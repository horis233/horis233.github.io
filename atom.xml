<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jiaming-hu.com</id>
    <title>Jiaming&apos;s Blog</title>
    <updated>2020-04-08T00:19:49.391Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jiaming-hu.com"/>
    <link rel="self" href="https://jiaming-hu.com/atom.xml"/>
    <subtitle>Whyâ€™s THE Design</subtitle>
    <logo>https://jiaming-hu.com/images/avatar.png</logo>
    <icon>https://jiaming-hu.com/favicon.ico</icon>
    <rights>All rights reserved 2020, Jiaming&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes Question -- day3]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day3/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day3/">
        </link>
        <updated>2020-04-07T09:58:16.000Z</updated>
        <content type="html"><![CDATA[<p>Create a deployment and expose Service with a single command. The deployment and service name is cka-1120, using nginx image, deployment has 2 pods</p>
<h2 id="answer">Answer</h2>
<pre><code>[root@liabio ~]# kubectl run  cka-1120 --replicas 2 --expose=true --port=80 --image=nginx

kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/cka-1120 created
deployment.apps/cka-1120 created

[root@liabio ~]# kubectl get all | grep cka-1120
pod/cka-1120-554b9c4798-7jcrb   1/1 Running 0118m
pod/cka-1120-554b9c4798-fpjwj   1/1 Running 0118m
service/cka-1120  ClusterIP 10.108.140.25 &lt;none&gt; 80/TCP           118m
deployment.apps/cka-11202/222118m
 
</code></pre>
<p>The official website provides detailed kubectl usage, located under REFERENCE-&gt;kubectl CLI-&gt;kubectl Commands tab.<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run</p>
<p>Command <code>kubectl run</code> will create a <code>deployment</code> or <code>job</code> to manage Pods. The command syntax is as follows:</p>
<pre><code>kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
</code></pre>
<p>NAME specifies the name of deployment and service;</p>
<p>--replicas abbreviation -r, specify the number of instances, the default is 1;</p>
<p>--expose if true, will create a ClusterIP service, the default is false;</p>
<p>--port indicates the port exposed by the container. If expose is true, the port is also the service port;</p>
<p>--image specifies the image used by the container;</p>
<p>--dry-run is true, only print the object to be sent, and not send it, the default is false.</p>
<p>Create a deployment named cka-1120-01 with environment variables</p>
<pre><code>kubectl run cka-1120-01 --image=nginx --env=&quot;DNS_DOMAIN=cluster.local&quot; --env=&quot;POD_NAMESPACE=default&quot;
</code></pre>
<p>Create a deployment named cka-1120-02 with label</p>
<pre><code>kubectl run cka-1120-02 --image=nginx --labels=&quot;app=nginx,env=prod&quot;
</code></pre>
<p>There is also a <code>--restart</code> parameter, the default is Always, if set to OnFailure, the job will be created; if set to Never, the ordinary Pod will be created.</p>
<pre><code>kubectl run cka-1120-03 --image=nginx --restart=OnFailure

kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
job.batch/cka-1120-03 created
</code></pre>
<pre><code>kubectl run cka-1120-04 --image=nginx --restart=Never

pod/cka-1120-04 created
</code></pre>
<p><code>--schedule</code> specifies the timing rule of cronjob, if this parameter is specified, cronjob will be created.</p>
<pre><code>kubectl run pi --schedule=&quot;0/5 * * * ?&quot; --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'

cronjob.batch/pi created
</code></pre>
<p>Currently, it is not supported to directly create resource objects such as Statefulset and Daemonset.</p>
<p><strong>What happens after kubectl run is executed?</strong></p>
<p>https://github.com/kubernetes/kubernetes/blob/master/cmd/clicheck/check_cli_conventions.go</p>
<pre><code>func main() {
	var errorCount int

	kubectl := cmd.NewKubectlCommand(os.Stdin, ioutil.Discard, ioutil.Discard)
	errors := cmdsanity.RunCmdChecks(kubectl, cmdsanity.AllCmdChecks, []string{})
	for _, err := range errors {
		errorCount++
		fmt.Fprintf(os.Stderr, &quot;     %d. %s\n&quot;, errorCount, err)
	}
</code></pre>
<p><code>cmd.NewKubectlCommand</code> is to build <code>kubectl</code> and its sub-command line parameters. The final code to execute the business logic is under the &quot;pkg\kubectl&quot; package. Different subcommands: apply, run, create entry corresponding to &quot;pkg\kubectl\cmd&quot;<br>
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go</p>
<pre><code>func NewCmdRun(f cmdutil.Factory, streams genericclioptions.IOStreams) *cobra.Command {
	o := NewRunOptions(streams)

	cmd := &amp;cobra.Command{
		Use:                   &quot;run NAME --image=image [--env=\&quot;key=value\&quot;] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]&quot;,
		DisableFlagsInUseLine: true,
		Short:                 i18n.T(&quot;Run a particular image on the cluster&quot;),
		Long:                  runLong,
		Example:               runExample,
		Run: func(cmd *cobra.Command, args []string) {
			cmdutil.CheckErr(o.Complete(f, cmd))
			cmdutil.CheckErr(o.Run(f, cmd, args))
		},
	}
</code></pre>
<p><code>o.Run(f, cmd, args)</code> will perform a series of checks on the parameters passed by kubectl run, filled with default values</p>
<pre><code>	var createdObjects = []*RunObject{}
	runObject, err := o.createGeneratedObject(f, cmd, generator, names, params, cmdutil.GetFlagString(cmd, &quot;overrides&quot;), namespace)
	if err != nil {
		return err
	}
	createdObjects = append(createdObjects, runObject)

	allErrs := []error{}
	if o.Expose {
		serviceGenerator := cmdutil.GetFlagString(cmd, &quot;service-generator&quot;)
		if len(serviceGenerator) == 0 {
			return cmdutil.UsageErrorf(cmd, &quot;No service generator specified&quot;)
		}
		serviceRunObject, err := o.generateService(f, cmd, serviceGenerator, params, namespace)
		if err != nil {
			allErrs = append(allErrs, err)
		} else {
			createdObjects = append(createdObjects, serviceRunObject)
		}
	}
</code></pre>
<p><code>o.generateService</code> Generate deployment, cronjob, job, pod and other resource objects according to different generators, and send creation requests to apiserver.</p>
<p>If expose is set to true, the same call o.createGeneratedObject is used to generate and create a service.</p>
<pre><code>func (o *RunOptions) createGeneratedObject(f cmdutil.Factory, cmd *cobra.Command, generator generate.Generator, names []generate.GeneratorParam, params map[string]interface{}, overrides, namespace string) (*RunObject, error) {
	err := generate.ValidateParams(names, params)
	if err != nil {
		return nil, err
	}

	// TODO: Validate flag usage against selected generator. More tricky since --expose was added.
	obj, err := generator.Generate(params)
	if err != nil {
		return nil, err
	}
</code></pre>
<p><code>generator.Generate(params)</code> generates different resource objects according to different generator implementations.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- day 2]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-2/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-2/">
        </link>
        <updated>2020-04-06T03:19:12.000Z</updated>
        <content type="html"><![CDATA[<p>Under the Kubernetes PVC + PV system, What components need to participate the volume plug-in implemented by CSI, from the pv being dynamically created to pv being used by pods?</p>
<p>A.</p>
<pre><code>PersistentVolumeController + CSI-Provisoner + CSI controller plugin
</code></pre>
<p>B.</p>
<pre><code>AttachDetachController + CSI-Attacher + CSI controller plugin
</code></pre>
<p>C.</p>
<pre><code>Kubelet + CSI node plugin
</code></pre>
<h2 id="answer-a-b-and-c">Answer: A, B and C</h2>
<p>K8s uses <code>PVC</code> to describe the size of the persistent storage that Pod wants to use, read and write permissions, etc., which are generally created by developers; <code>PV</code> describes specific storage types, storage addresses, mounting directories, etc., generally by Ops personnel create. Instead of writing the volume information directly in the pod. First, it can make the development and operation and maintenance responsibilities clear. Second, the use of <code>PVC</code> and <code>PV</code> mechanisms can be well extended to support different storage implementations on the market, such as k8s v1.10 version for Local Persistent Volume.</p>
<p>Let's try to understand the overall process from Pod creation to volume availability.</p>
<p>The user submits a request to create a pod, and <code>PersistentVolumeController</code> finds that the pod claims to use <code>PVC</code>, and it will help it find a <code>PV</code> pair.</p>
<p>If there is no ready-made <code>PV</code>, go to the corresponding <code>StorageClass</code>, help it create a new <code>PV</code>, and then complete the binding with <code>PVC</code>.</p>
<p>The newly created <code>PV</code> is still just an API object, and it needs to undergo &quot;two-stage processing&quot; before it can be used as the &quot;persistent volume&quot; on the host machine:</p>
<ul>
<li>
<p>In the first phase, <code>AttachDetachController</code> running on the master is responsible for attaching the <code>PV</code> and mounting the remote disk for the host;</p>
</li>
<li>
<p>The second stage is to run inside the kubelet component on each node, mount the remote disk attached in the first step to the host directory. This control loop is called <code>VolumeManagerReconciler</code>, which runs on a separate Goroutine and does not block the kubelet main control loop.</p>
</li>
</ul>
<p>After completing these two steps, the &quot;persistent volume&quot; corresponding to the PV is ready, and the POD can be started normally, and the &quot;persistent volume&quot; is mounted on the specified path in the container.</p>
<p>k8s supports writing its own storage plug-in <code>FlexVolume</code> and <code>CSI</code>. Either way, you need to go through &quot;two-stage processing.&quot; FlexVolume has more limitations than CSI. Generally, we use <code>CSI</code> to dock storage.</p>
<p>The design idea of â€‹â€‹the <code>CSI</code> plug-in system strips this Provision stage (dynamically creating <code>PV</code>) and part of the storage management functions in Kubernetes from the main code to make it into several separate components. These components will monitor changes in storage-related events in Kubernetes, such as the creation of <code>PVC</code>, through the Watch API to perform specific storage management actions.</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586270614137.png" alt="" loading="lazy"></figure>
<p>The three independent external components (External Components) in the CSI storage plug-in system in the above figure, namely: <code>Driver Registrar</code>, <code>External Provisioner</code> and <code>External Attacher</code>, correspond to some storage management functions stripped from the Kubernetes project.</p>
<p>We need to implement a binary of Custom Components, which will provide three services in gRpc mode: <code>CSI Identity</code>, <code>CSI Controller</code>, and <code>CSI Node</code>.</p>
<p><code>Driver Registrar</code> component, which is responsible for registering the plug-in in kubelet; <code>Driver Registrar</code> calls CSI Identity service to obtain plug-in information.</p>
<p>The <code>External Provisioner</code> component listens to the PVC objects in the APIServer. When a PVC is created, it will call the CreateVolume method of the CSI Controller to create the corresponding PV;</p>
<p>The <code>External Attacher</code> component is responsible for the Attach phase. The Mount phase is completed by the <code>VolumeManagerReconciler</code> control loop in the kubelet directly calling the CSI Node service.</p>
<p>After the two stages are completed, kubelet passes the mount parameter to docker to create and start the container.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- day1]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day1/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day1/">
        </link>
        <updated>2020-04-05T12:52:09.000Z</updated>
        <content type="html"><![CDATA[<p>Today's question:</p>
<p>Which of the following <code>Daemonset</code> yaml are correct:</p>
<p>A.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels: k8s-app: fluentd-logging 
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
        restartPolicy: Never
</code></pre>
<p>B.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata: 
      labels: 
        name: fluentd-elasticsearch
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Onfailure
</code></pre>
<p>C.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Always
</code></pre>
<p>D.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
</code></pre>
<h2 id="answer-c-and-d"><strong>Answer: C and D</strong></h2>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586221558403.jpeg" alt="" loading="lazy"></figure>
<p><code>RestartPolicy</code> field, optional values are <code>Always</code>, <code>OnFailure</code>, and <code>Never</code>. The default is Always. There can be multiple containers in a Pod, and restartPolicy applies to all containers in the Pod. The purpose of restartPolicy is to let kubelet restart the failed container.</p>
<p>In addition, the <code>restartPolicy</code> of <code>Deployment</code> and <code>Statefulset</code> must also be <code>Always</code> to ensure that the pod exits abnormally, or the health check livenessProbe fails and the kubelet restarts the container.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/deployment</p>
<p><code>Job</code> and <code>CronJob</code> are pods that run once, and <code>restartPolicy</code> can only be <code>OnFailure</code> or <code>Never</code>, to ensure that the container will not restart after the execution is completed.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Options Pattern in Go]]></title>
        <id>https://jiaming-hu.com/post/functional-options-pattern-in-go/</id>
        <link href="https://jiaming-hu.com/post/functional-options-pattern-in-go/">
        </link>
        <updated>2020-04-01T13:43:20.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<blockquote>
<p>reference <a href="https://halls-of-valhalla.org/beta/articles/functional-options-pattern-in-go,54/">functional-options-pattern-in-go</a></p>
</blockquote>
<p>One of the many issues you'll encounter as a Golang developer is trying to make parameters to a function optional. It's a pretty common use case that you have some object which should work out-of-the-box using some basic default settings, and you may occasionally want to provide some more detailed configuration.</p>
<p>In many languages this is easy; in C-family languages, you can provide multiple versions of the same function with different numbers of arguments; in languages like PHP, you can give parameters a default value and omit them when calling the method. But in Golang you can't do either of those. So how do you create a function which has some additional configuration a user can specify if they want, but only if they want to?</p>
<p>There are a number of possible ways to do this, but most are pretty unsatisfactory, either requiring a lot of additional checking and validating in the code on the service-side, or extra work for the client by passing in additional parameters which they don't care about.</p>
<p>I'll walk through some of the different options and show why each is suboptimal and then we'll build our way up to our final, clean solution: the Functional Options Pattern.</p>
<p>Let's take a look at an example. Let's say we have some service called StuffClient which does some stuff and has a two configuration options (timeout and retries):</p>
<pre><code>type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
</code></pre>
<p>That struct is private, so we should provide some sort of constructor for it:</p>
<pre><code>func NewStuffClient(conn Connection, timeout, retries int) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: timeout,
        retries: retries,
    }
}
</code></pre>
<p>Hmm, but now we always have to provide the timeout and retries every time we call <code>NewStuffClient</code>. And most of the time we'll want to just use the default values. We can't define multiple versions of <code>NewStuffClient</code> with different numbers of parameters or else we'll get a compile error like &quot;NewStuffClient redeclared in this block&quot;.</p>
<p>One option would be to create another constructor with a different name like:</p>
<pre><code>func NewStuffClient(conn Connection) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: DEFAULT_TIMEOUT,
        retries: DEFAULT_RETRIES,
    }
}
func NewStuffClientWithOptions(conn Connection, timeout, retries int) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: timeout,
        retries: retries,
    }
}
</code></pre>
<p>But that's kind of crappy. We can do better than that. What if we passed in a config object:</p>
<pre><code>type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func NewStuffClient(conn Connection, options StuffClientOptions) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: options.Timeout,
        retries: options.Retries,
    }
}
</code></pre>
<p>So what's the solution? The nicest way to solve this dilemma is with the Functional Options Pattern, making use of Go's convenient support of closures. Let's keep this StuffClientOptions we defined above, but we'll add some things to it:</p>
<pre><code>type StuffClientOption func(*StuffClientOptions)
type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func WithRetries(r int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Timeout = t
    }
}
</code></pre>
<p>Clear as mud right? What's happening here exactly? Basically we have our struct defining the available options for our StuffClient. Additionally now we've defined something called StuffClientOption (singular this time) which is just a function which accepts our options struct as a parameter. We've defined a couple of functions additionally called WithRetries and WithTimeout which return a closure. Now comes the magic:</p>
<pre><code>var defaultStuffClientOptions = StuffClientOptions{
    Retries: 3,
    Timeout: 2,
}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    options := defaultStuffClientOptions
    for _, o := range opts {
        o(&amp;options)
    }
    return &amp;stuffClient{
        conn:    conn,
        timeout: options.Timeout,
        retries: options.Retries,
    }
}
</code></pre>
<p>We've defined now an additional unexposed variable containing our default options, and we've adjusted our constructor now to instead accept a variadic parameter. We then iterate over that list of StuffClientOption (singular) and for each of them, we apply the returned closure to our options variable (and recall that those closures accept an StuffClientOptions variable and simply modify the option value on it).</p>
<p>Now all we have to do to use it is this:</p>
<pre><code>x := NewStuffClient(Connection{})
fmt.Println(x) // prints &amp;{{} 2 3}
x = NewStuffClient(
    Connection{},
    WithRetries(1),
)
fmt.Println(x) // prints &amp;{{} 2 1}
x = NewStuffClient(
    Connection{},
    WithRetries(1),
    WithTimeout(1),
)
fmt.Println(x) // prints &amp;{{} 1 1}
</code></pre>
<p>That looks pretty nice and usable now. And the nice part about it is that we can very easily add new options any time we want with only a very minimal amount of change we need to make to the code.</p>
<p>Putting it all together we have something like this:</p>
<pre><code>var defaultStuffClientOptions = StuffClientOptions{
    Retries: 3,
    Timeout: 2,
}
type StuffClientOption func(*StuffClientOptions)
type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func WithRetries(r int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Timeout = t
    }
}
type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
type Connection struct {}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    options := defaultStuffClientOptions
    for _, o := range opts {
        o(&amp;options)
    }
        return &amp;stuffClient{
            conn:    conn,
            timeout: options.Timeout,
            retries: options.Retries,
        }
}
func (c stuffClient) DoStuff() error {
    return nil
}
</code></pre>
<p>If you want to try it out yourself, check it out on the <a href="https://play.golang.org/p/VcWqWcAEyz">Go Playground</a>.</p>
<p>But this could be simplified even more by removing the StuffClientOptions struct and applying the options directly to our StuffClient.</p>
<pre><code>var defaultStuffClient = stuffClient{
    retries: 3,
    timeout: 2,
}
type StuffClientOption func(*stuffClient)
func WithRetries(r int) StuffClientOption {
    return func(o *stuffClient) {
        o.retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *stuffClient) {
        o.timeout = t
    }
}
type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
type Connection struct{}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    client := defaultStuffClient
    for _, o := range opts {
        o(&amp;client)
    }
    
    client.conn = conn
    return client
}
func (c stuffClient) DoStuff() error {
    return nil
}
</code></pre>
<p>Try it out <a href="https://play.golang.org/p/Z5P5Om4KDL">here</a>. In the case of our example, where we're just applying the config directly to our struct, it makes no sense to have an extra config struct in the middle. However, note that in many cases you may still want to use the config struct from the previous example; for instance, if your constructor is using the config options to perform some operations but then not storing them into the struct, or if they get passed around to other places. The config struct variant is the more generic implementation.</p>
<p>Credit to <a href="https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html">Rob Pike</a> and <a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">Dave Cheney</a> for popularizing this design pattern.</p>
]]></content>
    </entry>
</feed>