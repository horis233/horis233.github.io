<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jiaming-hu.com</id>
    <title>Jiaming&apos;s Blog</title>
    <updated>2020-04-18T00:28:32.331Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jiaming-hu.com"/>
    <link rel="self" href="https://jiaming-hu.com/atom.xml"/>
    <subtitle>Why’s THE Design</subtitle>
    <logo>https://jiaming-hu.com/images/avatar.png</logo>
    <icon>https://jiaming-hu.com/favicon.ico</icon>
    <rights>All rights reserved 2020, Jiaming&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 9 | (isolated from the network) to allow access to A B, C is not allowed to access B, how to do it]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-9-or-isolated-from-the-network-to-allow-access-to-a-b-c-is-not-allowed-to-access-b-how-to-do-it-k8s-access-control-rbac-role-rolebinding-and-lead-serviceaccount/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-9-or-isolated-from-the-network-to-allow-access-to-a-b-c-is-not-allowed-to-access-b-how-to-do-it-k8s-access-control-rbac-role-rolebinding-and-lead-serviceaccount/">
        </link>
        <updated>2020-04-16T20:24:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Deployment application deployment three (A, B, C), to allow access to B application A, application B but not allowed to access C.</p>
<ul>
<li>Deployment name for cka-1128-01, cka-1128-02, cka-1128-03</li>
<li>Network Policy name for cka-1128-np</li>
</ul>
<p><strong>Note:</strong> The command to create a complete deployment and network policy yaml, B and prove that A can access the application; C does not allow access B application. Divided into several comments.</p>
<h2 id="answer">Answer</h2>
<p>The first Deploy file cka-1128-01.yaml, using a radial/busyboxplusmirror because there are no curl busybox commands.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-01
spec:
  selector:
    matchLabels:
      app: cka-1128-01
  template:
    metadata:
      labels:
        app: cka-1128-01
    spec:
      containers:
        - name: cka-1128-01
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>CKA-1128-02.yaml:</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-02
spec:
  selector:
    matchLabels:
      app: cka-1128-02
  template:
    metadata:
      labels:
        app: cka-1128-02
    spec:
      containers:
        - name: cka-1128-02
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>CKA-1128-03.yaml:</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-03
spec:
  selector:
    matchLabels:
      app: cka-1128-03
  template:
    metadata:
      labels:
        app: cka-1128-03
    spec:
      containers:
        - name: cka-1128-03
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>You can see A, C can access the B:</p>
<pre><code class="language-bash">[root@liabio cka]# kubectl get pod -owide | grep cka
cka-1128-01-7b8b8cb79-mll6d        1/1     Running   0          3m5s   192.168.155.124   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-02-69dd65bdb7-mfq26       1/1     Running   0          3m8s   192.168.155.117   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-03-66f8f69ff-64q75        1/1     Running   0          3m3s   192.168.155.116   liabio   &lt;none&gt;           &lt;none&gt;
[root@liabio cka]# kubectl exec -ti cka-1128-01-7b8b8cb79-mll6d  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.146 ms
64 bytes from 192.168.155.117: seq=1 ttl=63 time=0.095 ms
[root@liabio cka]# kubectl exec -ti cka-1128-03-66f8f69ff-64q75  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.209 ms
64 bytes from 192.168.155.117: seq=1 ttl=63 time=0.112 ms
</code></pre>
<p>New cka-1128-np.yaml, <code>kubectl apply -f cka-1128-np.yaml</code> creating <code>Network Policy</code>, spec.podSelector.matchLabels select Pod B management; ingress.from.podSelector.matchLabels only specify the flow rate from the opening A whitelist.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cka-1128-np
spec:
  podSelector:
    matchLabels:
      app: cka-1128-02
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: cka-1128-01
</code></pre>
<p>A verification discovery can ping B, C can not ping B.</p>
<pre><code class="language-bash">[root@liabio cka]# kubectl apply -f cka-1128-np.yaml 
networkpolicy.networking.k8s.io/cka-1128-np created
[root@liabio cka]# kubectl get networkpolicies.
NAME          POD-SELECTOR      AGE
cka-1128-np   app=cka-1128-02   13s
[root@liabio cka]# 
[root@liabio cka]# kubectl get pod -owide | grep cka
cka-1128-01-7b8b8cb79-mll6d        1/1     Running   1          24m    192.168.155.124   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-02-69dd65bdb7-mfq26       1/1     Running   1          24m    192.168.155.117   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-03-66f8f69ff-64q75        1/1     Running   1          24m    192.168.155.116   liabio   &lt;none&gt;           &lt;none&gt;
[root@liabio cka]# kubectl exec -ti cka-1128-01-7b8b8cb79-mll6d  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.213 ms
[root@liabio cka]# kubectl exec -ti cka-1128-03-66f8f69ff-64q75  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
......
</code></pre>
<h2 id="analysis">Analysis</h2>
<p>The key point of this question is to examine k8s network policy NetworkPolicy, this knowledge is also often asked in the interview. The following excerpt from the official document:</p>
<h3 id="concept">Concept</h3>
<p>Network Policy (NetworkPolicy) is a specification about communication between the pod and pod and other rules between network endpoints are allowed.<br>
NetworkPolicy tag selector resource pod, and the pod is allowed select a definition of communication rules. Network policy implemented by the network plug-in, so users must use a network solution supports the NetworkPolicy,</p>
<p>Network policy concept with the introduction of the official document:<br>
https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource</p>
<p>Network policy practice official documents, this title can refer to the documentation is complete.<br>
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/</p>
<h3 id="premise">premise</h3>
<p>Network policy implemented by the network plug-in, so users must use a network solution supports NetworkPolicy of - simply create resource objects, but no controller to take effect, then, there is no effect.</p>
<p>###Supported network solutions<br>
Calico、 Kube-router、 Cilium、Romana、Weave Net</p>
<p>k8s official documentation:<br>
https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/</p>
<h3 id="isolated-and-non-isolated-pod">Isolated and non-isolated Pod</h3>
<p>By default, Pod non-isolated, they receive from any source.</p>
<p>Pod can be isolated by the relevant network policy. Once the namespace has NetworkPolicy selected a specific Pod, the Pod will refuse to connect to the network strategy not allowed. (Other network policy is not selected Pod will continue to receive all the traffic in the namespace)</p>
<h3 id="networkpolicy-resources">NetworkPolicy resources</h3>
<p>Here is an example of a NetworkPolicy:</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
</code></pre>
<p>Unless the Select Network solution supports NetworkPolicy, otherwise it will be sent to the above example APIServer no effect.</p>
<p>spec: NetworkPolicy spec contains all the information needed to define a specific network policy in a namespace</p>
<p>podSelector: Each NetworkPolicy comprises a podSelector, it is selected the group policy applies Pod. Because NetworkPolicy currently only supports ingress define rules for the policy is the definition of &quot;target pod&quot; on podSelector essence here. Strategies example with pod &quot;role = db&quot; tag. Empty podSelector select all pod under the namespace.</p>
<p>policyTypes : Each NetworkPolicy policyTypes contains a list, which may contain Ingress, Egress or both. policyTypes field indicates whether a given policy applies to inbound traffic of the selected Pod, Pod outbound traffic from a selected, or both are applicable. If you do not specify any policyType on NetworkPolicy, the default is always set Ingress, and if NetworkPolicy have any export rules, then the Egress settings.</p>
<p>ingress: List each NetworkPolicy ingress rule contains a white list. (Them) from the rule matching to allow simultaneous flow ports and portions. Example policy includes a simple rule: it matches a single Port, from one of two sources, the first specified by namespaceSelector, designated by the second podSelector.</p>
<p>egress: a list of every NetworkPolicy egress rule contains a white list. Each rule allowed to match and flow port section. The example policy contains a rule that matches the traffic on a single port to any destination of 10.0.0.0/24.</p>
<p>So, the example network policy:</p>
<p>Under isolation &quot;default&quot; namespace &quot;role = db&quot; of the pod (if they are not already isolated words).<br>
Allow pod with the &quot;role = frontend&quot; tag from the &quot;default&quot; namespace 6379 is connected to the TCP port of the pod &quot;default&quot; in the namespace.</p>
<p>&quot;Default&quot; namespace, Pod labeled with any &quot;role = frontend&quot;; and<br>
namespaces in the pod with any label &quot;project = myproject&quot;; and<br>
172.17.0.0-172.17.0.255 range of IP addresses and<br>
172.17.2.0-172.17.255.255 (i.e., all except 172.17.1.0/24 172.17.0.0/16) of<br>
Allow with the &quot;project = myproject&quot; pod at any namespace tag 6379 is connected to the TCP port of the &quot;default&quot; under the name space pod.</p>
<h3 id="selector-to-and-from-the-behavior">Selector to and from the behavior</h3>
<p>Four kinds of selectors can be specified in the ingress from section to section or egress:</p>
<p>podSelector: This selects a specific Pod NetworkPolicy with the same name space, the inlet should be permitted as a source or destination outlet.</p>
<p>namespaceSelector: This selects a particular namespace, all Pod should be used as an input source or output destination.</p>
<p>namespaceSelector and podSelector: a designated to namespaceSelector and the podSelector / from a particular entry selection namespace specific Pod. Note the use of the correct YAML syntax; this strategy:</p>
<p>...<br>
ingress:</p>
<ul>
<li>from:
<ul>
<li>namespaceSelector:<br>
matchLabels:<br>
user: alice<br>
podSelector:<br>
matchLabels:<br>
role: client<br>
...<br>
From the array contains only one element, only marked role = client from the name space of the Pod Pod and labeled where user = alice connection. This strategy:</li>
</ul>
</li>
</ul>
<p>...<br>
ingress:</p>
<ul>
<li>from:
<ul>
<li>namespaceSelector:<br>
matchLabels:<br>
user: alice</li>
<li>podSelector:<br>
matchLabels:<br>
role: client<br>
...<br>
Contained in two elements from the array, from the local namespace allows marked role = Pod connection of the client, or from any namespace labeled user = alice any connection of the Pod.</li>
</ul>
</li>
</ul>
<p>ipBlock: This selects a particular IP CIDR range as an inlet or source outlet destinations. These should be outside the cluster IP, because time is short Pod IP presence and randomly generated.</p>
<p>Inlet and outlet mechanisms usually need to rewrite the cluster IP source or destination IP packet. In the case of this happening, uncertainty occurs before or after NetworkPolicy processing and plug-ins for different combinations of networks, cloud providers, Service realization, the behavior may be different.</p>
<p>In the case of entry, which means that in some cases, you can filter incoming packets based on the actual source of the original IP, while in other cases, NetworkPolicy the role of the IP source or it may be LoadBalancer Pod node Wait.</p>
<p>For exports, which means that from the Pod to be rewritten as a cluster of Service IP connectivity to external IP may or may not be bound by the policy-based ipBlock.</p>
<h3 id="the-default-policy">The default policy</h3>
<p>By default, if there is no policy namespace, all traffic in and out of the namespace Pod will be allowed. The following example lets you change the default behavior of the name space.</p>
<h3 id="default-deny-all-traffic-entry">Default deny all traffic entry</h3>
<p>You can select all containers but not by creating any NetworkPolicy inlet flow into the container to create these &quot;default&quot; isolation strategy for the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
</code></pre>
<p>This ensures that even if the container NetworkPolicy no other choice, it can still be isolated. This policy will not change the default behavior export isolation.</p>
<p>The default allows all ingress traffic<br>
If you want to allow all traffic entering all Pod a namespace (even if you add the policy causes some Pod is considered &quot;isolated&quot;), you can create a policy to explicitly permit all traffic to the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
</code></pre>
<p>Default Deny all export traffic<br>
You can select all container exports but not by creating any traffic NetworkPolicy from these containers to create a &quot;default&quot; egress isolation strategy for the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Egress
</code></pre>
<p>This ensures that even if not any other choice NetworkPolicy Pod will not be allowed out of traffic. This policy will not change the default ingress isolation behavior.</p>
<p>The default allows all export traffic<br>
If you want to allow all traffic from the namespace of all Pod (even if you add a policy causes some Pod is considered &quot;isolated&quot; in), you can create a policy that explicitly permits all traffic exit the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress
</code></pre>
<p>Default Deny all entry and exit of all traffic<br>
You can create a &quot;default&quot; policy for the namespace, by creating the following NetworkPolicy in the namespace block all inbound and outbound traffic.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
</code></pre>
<p>This ensures that even if not any other choice NetworkPolicy Pod will not be allowed into or out of traffic.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang Learning Blog List]]></title>
        <id>https://jiaming-hu.com/post/golang-learning-blog/</id>
        <link href="https://jiaming-hu.com/post/golang-learning-blog/">
        </link>
        <updated>2020-04-15T14:49:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="basic">Basic</h2>
<p><a href="http://devs.cloudimmunity.com/gotchas-and-common-mistakes-in-go-golang/">50 Shades of Go: Traps, Gotchas, and Common Mistakes for New Golang Devs</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 8 | Secret type, creation, Pod use, and ServiceAccount associated Secret]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-8/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-8/">
        </link>
        <updated>2020-04-13T23:52:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create a secret named cka1127-secret, which contains a password field with a value of cka1127, and then use ENV to call in Pod1 named cka1127-01, and use Volume to mount under /data in Pod2 named cka1127-02;</p>
<h2 id="answer">Answer</h2>
<p>###Create secret</p>
<h4 id="use-yaml-file">use YAML file</h4>
<p>cka1127-secret.yaml:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: cka1127-secret
type: Opaque
stringData:
  password: cka1127
</code></pre>
<pre><code>kubectl apply -f  cka-1127-secret.yaml 
</code></pre>
<h4 id="using-cli">using CLI</h4>
<pre><code>kubectl create secret generic cka1127-secret --from-literal=password=cka1127
</code></pre>
<h3 id="create-pod-cka1127-01">Create pod cka1127-01</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka1127-01
spec:
  containers:
  - image: nginx
    name: nginx
    env:
    - name: PASSWORD
       valueFrom:
        secretKeyRef:
          name: cka1127-secret
          key: password
</code></pre>
<pre><code>kubectl exec -ti cka1127-01  bash
root@cka1127-01:/# 
root@cka1127-01:/# echo $PASSWORD
cka1127
root@cka1127-01:/#
</code></pre>
<h3 id="create-pod-cka1127-02">Create pod cka1127-02</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka1127-02
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: pwd
       mountPath: /data
       readOnly: true
  volumes:
  - name: pwd
     secret:
       secretName: cka1127-secret
</code></pre>
<pre><code># kubectl exec -ti cka1127-02 sh
# 
# ls -l /data
total 0
lrwxrwxrwx 1 root root 15 Nov 28 00:40 password -&gt; ..data/password
# cat /data/password
cka1127# 
</code></pre>
<h2 id="introduction">Introduction</h2>
<p>K8s offical document of secret https://kubernetes.io/docs/concepts/configuration/secret/</p>
<h3 id="introduction-to-secret">Introduction to secret</h3>
<p><code>Secret</code> resources in Kubernetes can be used to store sensitive data such as passwords, tokens, and secret keys, and store these sensitive information in <code>Secret</code>, which is more secure and flexible than exposure to Pods and images.</p>
<p>You may think that secret is generally not used. In fact, when creating a Pod, Kubernetes automatically creates a secret containing the credentials used to access the API (controlled by the <code>service account token controller</code> of kube-controller-manager), and it will Automatically modify the Pod to use this type of secret (this is controlled by the <code>Admission Controller</code>).</p>
<p>Each namespace of k8s will have a Service Account. When we create a namespace, the service account controller will monitor the creation of the namespace, a service account named default will be created in the namespace, and the service account token controller will monitor the creation of the service account , Create the corresponding secret, and bind this secret to the Service Account.</p>
<pre><code># kubectl create ns cka
namespace/cka created
# kubectl get sa -n cka
NAME      SECRETS   AGE
default   1         11s
# kubectl get secrets -n cka
NAME                  TYPE                                  DATA   AGE
default-token-r77xn   kubernetes.io/service-account-token   3      18s
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586911624680.png" alt="" loading="lazy"></figure>
<p>When we create a Pod, if no Service Account is specified, it will automatically be assigned a Service Account in the same command space namespace by default. You can see that the spec.serviceAccountName field has been automatically set.</p>
<figure data-type="image" tabindex="2"><img src="https://jiaming-hu.com/post-images/1586911645342.jpeg" alt="" loading="lazy"></figure>
<p>You can use the automatically added Service Account credentials to access the API from within the Pod. The API permissions of the Service Account depend on the authorization plugin and policy used.</p>
<p>For more information about how Service Account works, please see the documentation: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</p>
<h3 id="types-of-secret">Types of Secret</h3>
<p><code>--type</code> specifies the type of secret created, Kubernetes has built-in three types of Secret</p>
<h4 id="kubernetesio-service-account-token-secret">kubernetes.io / service-account-token Secret</h4>
<p>As mentioned above, in order to access the Kubernetes API from within the Pod, Kubernetes provides Service Account resources. The Service Account will automatically create and mount the Secret that accesses the Kubernetes API, and will be mounted in the Pod's /var/run/secrets/kubernetes.io/serviceaccount directory. The Secret used by the Service Account that is automatically created when the namespace is created is of this type.<br>
<img src="https://jiaming-hu.com/post-images/1586911898128.png" alt="" loading="lazy"></p>
<h4 id="opaque-secret">Opaque Secret</h4>
<p>The Opaque type Secret is a map structure (key-value), where vlaue requires encoding in base64 format. In the following examples, it is basically the Opaque type Secret.</p>
<h4 id="kubernetesio-dockerconfigjson-secret">kubernetes.io / dockerconfigjson Secret</h4>
<p>Secret of type <code>kubernetes.io/dockercfg</code> is used to store private Docker Registry authentication information. When Kubernetes creates a Pod and needs to pull the image from the private Docker Registry, it needs to use the authentication information, and it will use the kubernetes.io/dockercfg type Secret.</p>
<pre><code>kubectl create secret docker-registry cka-regsecret \
--docker-server=coderaction \
--docker-username=admin \
--docker-password=123456 \
--docker-email=837448191@qq.com
</code></pre>
<p>When creating a Pod, you need to use it in the Pod spec as follows:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka-private-reg
spec:
  containers:
    - name: cka-private-reg-container
      image: nginx
  imagePullSecrets:
    - name: cka-regsecret
</code></pre>
<p>You can refer to the official documentation: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod</p>
<p>Add imagePullSecrets to the default ServiceAccount of the partition, so that all Pods created can automatically add spec.imagePullSecrets. For details, refer to the official document: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ # add-imagepullsecrets-to-a-service-account</p>
<h3 id="create-secret">Create secret</h3>
<h4 id="use-kubectl-to-create-secret">Use kubectl to create secret</h4>
<p>kubectl creates seccret official document: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-secret-em-</p>
<p>format</p>
<pre><code>kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run]
</code></pre>
<p>--from-literal: Specify the key and text value to be inserted (that is, mykey = somevalue), the value is a plain text value, and it will be encoded by base64 after creation; Files, in this case, will be assigned a default name; or you can choose to use the specified directory, which will iterate over each valid file key in that directory. --type: The type of secret created. Three types have been introduced above; --dry-run: If true, only the created object will be sent to APIServer without sending it. The default value is false</p>
<h3 id="create-secret-by-from-file">create secret by <code>--from-file</code></h3>
<pre><code># echo -n 'admin' &gt; ./username
# echo -n 'test123' &gt; ./password
# echo -n 'shanghai' &gt; ./city
# ll
total 12
-rw-r--r-- 1 root root 9 Nov 28 20:44 city
-rw-r--r-- 1 root root 8 Nov 28 20:43 password
-rw-r--r-- 1 root root 6 Nov 28 20:43 username
# kubectl create secret generic test-cka1127-01 --from-file=./username --from-file=./password
secret/test-cka1127-01 created
# kubectl create secret generic test-cka1127-02 --from-file=./
secret/test-cka1127-02 created
# 
</code></pre>
<p>With the secret created in the specified directory, all files in the directory are added to the data:</p>
<pre><code>kubectl get secrets test-cka1127-02 -oyaml
apiVersion: v1
data:
  city: c2hhbmdoYWkK
  password: dGVzdDEyMwo=
  username: YWRtaW4K
kind: Secret
metadata:
  creationTimestamp: &quot;2019-11-28T12:44:57Z&quot;
  name: test-cka1127-02
  namespace: default
  resourceVersion: &quot;14636360&quot;
  selfLink: /api/v1/namespaces/default/secrets/test-cka1127-02
  uid: 4a3a1a5d-09e6-4bf9-bbe3-3300db1ddf7a
type: Opaque
</code></pre>
<p>Specify the secret created by username and password:</p>
<pre><code>kubectl get secrets test-cka1127-01 -oyaml
apiVersion: v1
data:
  password: dGVzdDEyMwo=
  username: YWRtaW4K
kind: Secret
metadata:
  creationTimestamp: &quot;2019-11-28T12:44:47Z&quot;
  name: test-cka1127-01
  namespace: default
  resourceVersion: &quot;14636347&quot;
  selfLink: /api/v1/namespaces/default/secrets/test-cka1127-01
  uid: 766516a2-34be-4a18-b4e2-83751a6cd2b7
type: Opaque
</code></pre>
<h3 id="create-secret-from-the-generator">Create Secret from the generator</h3>
<p>Starting from 1.14, Kubectl supports Kustomize to manage objects. With this new feature, you can also create a Secret from the generator and then apply it to create objects on Apiserver.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 7 | InitContainer concept, usage, usage scenario introduction; k8s secret env, volume exam questions]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-7/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-7/">
        </link>
        <updated>2020-04-11T00:49:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Provide a yaml of the pod, request to add Init Container, the role of Init Container is to create an empty file, pod container to determine whether the file exists, otherwise, exit.</p>
<h2 id="answer">Answer</h2>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    run: cka-1126
  name: cka-1126
spec:
  initContainers:
  - image: busybox
    name: init-c
    command: ['sh', '-c', 'touch /tmp/cka-1126']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  containers:
  - image: busybox
    name: cka-1126
    command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  volumes:
  - name: workdir
    emptyDir: {}
</code></pre>
<p>The command of the Container is to determine whether the file exists, not exit if it exists, and exit if it does not exist; you can also use the following if to determine:</p>
<pre><code>command: ['sh', '-c', 'if [ -e /tmp/cka-1126 ];then echo &quot;file exits&quot;;else echo &quot;file not exits&quot; &amp;&amp; exit 1;fi']
</code></pre>
<h3 id="init-container">init container</h3>
<p>The key point of this question is that the init container and the main container need to mount a directory called workdir together. The init container creates an empty file in it. The main container checks whether the file exists, and the main syntax is shell syntax;</p>
<pre><code>command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
</code></pre>
<p>This sentence means: If the return code of <code>ls /tmp/cka-1126</code> is 0, that is the file exists, sleep 3600 seconds; otherwise exit 1 exits;</p>
<p>You can also use the <code>if</code> syntax to judge.</p>
<p>Official document address:<br>
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/<br>
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</p>
<h3 id="carding-concepts">Carding concepts</h3>
<p>Initialize the container. As the name suggests, when the container is started, it will start one or more containers. If there are multiple containers, then these Init Containers are executed in order according to the defined order. If one is executed successfully, the next one can be executed. Only all Init After the Container is executed, the main container will start. Since the storage volume in a Pod is shared, the data generated in the Init Container can be used by the main container.</p>
<p>Init Container can be used in a variety of K8S resources such as Deployment, Daemon Set, StatefulSet, Job, etc. But in the final analysis, it is executed when the Pod starts, before the main container starts, and does the initialization work.</p>
<p>The Init container supports all fields and features of the application container, including resource limits, data volumes, and security settings. However, Init containers do not support Readiness Probes because they must be completed before the Pod is ready; there will be slight differences in resource limits and scheduling.</p>
<p>###Application scenario<br>
Wait for other modules Ready: For example, there is an application with two containerized services, one is Web Server and the other is database. The Web Server needs to access the database. However, when we start this application, there is no guarantee that the database service will start first, so there may be errors in the Web Server connecting to the database for a period of time. In order to solve this problem, we can use an InitContainer in the Pod running the Web Server service to check whether the database is ready, until the database can be connected, the Init Container ends and exits, and then the Web Server container is started to initiate a formal database connection request .</p>
<p>Initial configuration: for example, to detect all existing member nodes in the cluster and prepare the configuration information of the cluster for the main container, so that the main container can use this configuration information to join the cluster when it is up; currently in containerization, it is often used when initializing the cluster configuration file Arrive</p>
<p>Provide a way to block the start of the container: the next container must be run after the initContainer container is successfully started, which guarantees a successful way of running a set of conditions;</p>
<p>Other usage scenarios: register pods to a central database, download application dependencies, etc.</p>
<p>Kubernetes version 1.5 began to support pod.beta.kubernetes.io/init-containers to declare initContainer under annotations, like the following.</p>
<pre><code>
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
  annotations:
    pod.beta.kubernetes.io/init-containers: '[
        {
            &quot;name&quot;: &quot;init-myservice&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&quot;]
        },
        {
            &quot;name&quot;: &quot;init-mydb&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&quot;]
        }
    ]'
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
</code></pre>
<p>The new syntax of Kubernetes version 1.6 moved the declaration of the Init container to spec, but the old annotation syntax can still be used.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 6 | Deployment upgrade, rollback, rolling update strategy, roll, set image command explanation, initContainer exam questions]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-6/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-6/">
        </link>
        <updated>2020-04-10T13:32:51.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Through the command line, create a deployment, the number of copies is 3, and the image is nginx: latest. Then scroll to nginx: 1.9.1, and then roll back to the original version</p>
<p>Requirements: The name of the deployment is cka-1125, and the related commands used are posted.</p>
<p>It is best to attach the completed deployment yaml and the commands related to the upgrade rollback.</p>
<h2 id="anwser">Anwser</h2>
<p>Create a deployment first, you can use the command to create:</p>
<pre><code>kubectl run cka-1125  --image=nginx --replicas=3
</code></pre>
<p>You can also use the following yaml: cka-1125.yaml to create</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1125
  name: cka-1125
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cka-1125
  template:
    metadata:
      labels:
        app: cka-1125
    spec:
      containers:
      - image: nginx
        name: cka-1125
</code></pre>
<p>Create:</p>
<pre><code>kubectl apply -f cka-1125.yaml
</code></pre>
<p>Upgrade:</p>
<pre><code>kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
</code></pre>
<p>Rollback:</p>
<pre><code># Rollback to the previous version
kubectl rollout undo deploy/cka-1125
# Rollback to the specified version
kubectl rollout undo deploy/cka-1125 --to-revision=2
</code></pre>
<p>the offical document of the set image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set</p>
<h3 id="set-image">Set image</h3>
<p>Set image command:<br>
The format of the set image command is as follows:</p>
<pre><code>kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [--record]
</code></pre>
<p><code>--record</code> specifies that the current kubectl command is recorded in the annotation. If set to <code>false</code>, the command is not recorded. If set to <code>true</code>, the command is recorded. The default is <code>false</code>.</p>
<pre><code>[root@liabio test]# kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
[root@liabio test]# 
[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
3         &lt;none&gt;
4         kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record=true
</code></pre>
<p>As above, there will be an upgrade command in CHANGE-CAUSE.<br>
The set image command can operate on: <code>pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), replicaset (rs), statefulset (sts)</code>.</p>
<h3 id="roll-command">Roll command</h3>
<p>the offical document of the roll image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout<br>
The roll command can manage the rollback of deployments, daemonsets, statefulsets resources:</p>
<p>Query upgrade history:</p>
<pre><code>[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
</code></pre>
<p>Check the details of the specified version:</p>
<pre><code>kubectl rollout history deploy/cka-1125 --revision=3 -o=yaml
</code></pre>
<p>Rollback to the previous version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Rollback to the specified version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 --to-revision=3
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Other roll subcommands:</p>
<ul>
<li>
<p>restart: the resource will restart; status: show the rollback status;</p>
</li>
<li>
<p>resume: resume the suspended resource. The controller does not control the suspended resources. By restoring resources, the controller can be controlled again. resume is only supported for deployment.</p>
</li>
<li>
<p>pause: The controller will not control the paused resource. Use kubectl rollout resume to resume suspended resources. Currently, only deployment support is suspended.</p>
</li>
</ul>
<h3 id="rolling-update-strategy">Rolling update strategy</h3>
<p>the offical document of rollingupdate<br>
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p>
<pre><code>minReadySeconds: 5
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
</code></pre>
<p><code>minReadySeconds</code> Kubernetes upgrades after waiting for the set time. If this value is not set, Kubernetes will assume that the container will provide services after it is started. If this value is not set, it may cause the service service to run normally in some extreme cases.</p>
<p><code>maxSurge</code> controls the total number of copies in the rolling update process to exceed the upper limit of DESIRED. maxSurge can be a specific integer or a percentage, rounded up. The default value of maxSurge is 25%.</p>
<p>For example, DESIRED is 10, then the maximum number of copies is roundUp (10 + 10 * 25%) = 13, so CURRENT is 13.</p>
<p><code>maxUnavaible</code> controls the maximum percentage of unavailable copies in DESIRED during the rolling update process. maxUnavailable can be a specific integer or 100%, rounded down. The default value is 25%.</p>
<p>For example, DESIRED is 10, then the number of available copies must be at least 10-roundDown (10 * 25%) = 8, so AVAILABLE is 8.</p>
<p><code>The larger maxSurge, the more new copies created initially; the larger maxUnavailable, the more old copies originally destroyed.</code></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day5 | Advanced scheduling of kube-scheduler scheduler: affinity anti-affinity scheduling, deployment upgrade rollback]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day5/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day5/">
        </link>
        <updated>2020-04-09T12:27:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>By using the command line, create two deployments.</p>
<p>There are 2 nodes in the cluster:</p>
<p>The first deployment name is cka-1122-01, using nginx image, there are 2 pods, and configure the deployment itself to anti-affinity at the node level between pods;</p>
<p>The second deployment name is cka-1122-02, using the nginx image, there are 2 pods, and configure the deployment pod have affinity with the first deployment pod at the node level;</p>
<h2 id="answer">Answer</h2>
<ul>
<li>The first deployment: cka-1122-01</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-01
  name: cka-1122-01
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-01
  template:
    metadata:
      labels:
        app: cka-1122-01
    spec:
      containers:
      - image: nginx
        name: cka-1122-01  
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cka-1122-01
              topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<ul>
<li>The second deployment: cka-1122-02</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-02
  name: cka-1122-02
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-02
  template:
    metadata:
      labels:
        app: cka-1122-02
    spec:
      containers:
      - image: nginx
        name: cka-1122-02
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cka-1122-01
            topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<p>The schedule result:</p>
<pre><code>NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
cka-1122-01-5df9bdf8c9-qwd2v    1/1      Running      0       8m     10.192.4.2     node-1
cka-1122-01-5df9bdf8c9-r4rhs    1/1      Running      0       8m     10.192.4.3     node-2  
cka-1122-02-749cd4b846-bjhzq    1/1      Running      0       10m    10.192.4.4     node-1
cka-1122-02-749cd4b846-rkgpo    1/1      Running      0       10m    10.192.4.5     node-2  
</code></pre>
<p>Affinity and Anti-Affinity document:<br>
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 4 | Manual scheduling, kube-scheduler scheduler analysis, source code analysis]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day4/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day4/">
        </link>
        <updated>2020-04-07T18:19:39.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Through the command line, use the nginx image to create a pod and manually schedule it to a node named node1121. The name of the pod is cka-1121. The best answer is attached. The command used to create the pod is the most streamlined yaml.<br>
if the comment is there Restrictions, please list the points of attention, mainly need to list how to do manual scheduling?</p>
<p>Note: Manual scheduling means that there is no need to go through kube-scheduler to schedule.</p>
<h2 id="answer">Answer</h2>
<p>Schedule the Pod named cka-1121 to node node 1121:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka-1121
  labels:
    app: cka-1121
spec:
  containers:
  - name: cka-1121
    image: busybox
    command: ['sh', '-c', 'echo Hello CKA! &amp;&amp; sleep 3600']
  nodeName: node1121
</code></pre>
<p>Address of the scheduler in the official website:</p>
<p>https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/</p>
<p>Scheduler command line parameters:</p>
<p>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/</p>
<p>The scheduler <code>kube-scheduler</code> is divided into <code>pre-selection</code>, <code>optimization</code>, <code>pod priority preemption</code>, and <code>bind</code> phase;</p>
<p><code>Pre-selection</code>: Pop the pods that need to be scheduled from the queue of podQueue to be scheduled. Enter the pre-selection stage first. The pre-selection function determines whether each node is suitable for scheduling by the pod.</p>
<p><code>Preferably</code>: select the optimal node from the satisfied nodes selected by the pre-selection.</p>
<p><code>Pod priority preemption</code>: If the pre-selection and preferred scheduling fail, it will try to remove the pods with low priority and make the pods with high priority scheduled successfully.</p>
<p><code>bind</code>: After the above steps are completed, the scheduler will update the local cache, but in the end, you need to submit the binding result to etcd, you need to call the Bind interface of Apiserver to complete.</p>
<blockquote>
<p>The following k8s source code version is 1.18.1<br>
Let's check the kube-scheduler source code. The scheduler uses the list-watch mechanism to monitor the Pod's new, updated, and deleted events in the cluster, and calls the callback function. After the nodeName is specified, it will not be put into the unscheduled podQueue queue, and it will not go through the above stages.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/scheduler.go#L234<br>
Among them, when constructing callback functions for adding, updating, and deleting pod resource objects, divided into scheduled and unscheduled callbacks.</p>
</blockquote>
<p>Scheduled pod callback:</p>
<p>The scheduled pods are filtered according to the logic defined in FilterFunc. When nodeName is not empty, when it returns true, it will go to AddFunc, UpdateFunc, and DeleteFunc defined in Handler. In the cache, because the scheduler maintains a cache of pod lists on the node.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L356</p>
<pre><code>	// scheduled pod cache
	podInformer.Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return assignedPod(t)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return assignedPod(pod)
					}
					utilruntime.HandleError(fmt.Errorf(&quot;unable to convert object %T to *v1.Pod in %T&quot;, obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf(&quot;unable to handle object in %T: %T&quot;, sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)
</code></pre>
<p>Unscheduled pod callbacks:</p>
<p>Unscheduled pods are filtered according to the logic defined in FilterFunc. When nodeName is empty and pod ’s SchedulerName matches the scheduler ’s name, true is returned; when true is returned, AddFunc, UpdateFunc, and DeleteFunc defined in Handler will be used. Will be added to podQueue.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L380</p>
<pre><code>	// unscheduled pod queue
	podInformer.Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return !assignedPod(t) &amp;&amp; responsibleForPod(t, sched.Profiles)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return !assignedPod(pod) &amp;&amp; responsibleForPod(pod, sched.Profiles)
					}
					utilruntime.HandleError(fmt.Errorf(&quot;unable to convert object %T to *v1.Pod in %T&quot;, obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf(&quot;unable to handle object in %T: %T&quot;, sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToSchedulingQueue,
				UpdateFunc: sched.updatePodInSchedulingQueue,
				DeleteFunc: sched.deletePodFromSchedulingQueue,
			},
		},
	)
</code></pre>
<ul>
<li>
<p>Applicable scenarios for manual scheduling:</p>
<p>When the scheduler is not working, you can set nodeName for temporary emergency;<br>
Can be packaged into its own scheduler;</p>
</li>
<li>
<p>Extension point:</p>
<p>In the past several versions of Daemonset, the controller directly specified the pod's running node without going through the scheduler.<br>
Until version 1.11, the pods of DaemonSet were scheduled by scheduler before being introduced as alpha features</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 3 | Proficiency in kubectl commands for creating resource objects and analyzing from source code]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day3/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day3/">
        </link>
        <updated>2020-04-04T21:58:16.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create a deployment and expose Service with a single command. The deployment and service name is cka-1120, using nginx image, deployment has 2 pods</p>
<h2 id="answer">Answer</h2>
<pre><code>[root@liabio ~]# kubectl run  cka-1120 --replicas 2 --expose=true --port=80 --image=nginx

kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/cka-1120 created
deployment.apps/cka-1120 created

[root@liabio ~]# kubectl get all | grep cka-1120
pod/cka-1120-554b9c4798-7jcrb   1/1 Running 0118m
pod/cka-1120-554b9c4798-fpjwj   1/1 Running 0118m
service/cka-1120  ClusterIP 10.108.140.25 &lt;none&gt; 80/TCP           118m
deployment.apps/cka-11202/222118m
 
</code></pre>
<p>The official website provides detailed kubectl usage, located under REFERENCE-&gt;kubectl CLI-&gt;kubectl Commands tab.<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run</p>
<p>Command <code>kubectl run</code> will create a <code>deployment</code> or <code>job</code> to manage Pods. The command syntax is as follows:</p>
<pre><code>kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
</code></pre>
<p>NAME specifies the name of deployment and service;</p>
<p>--replicas abbreviation -r, specify the number of instances, the default is 1;</p>
<p>--expose if true, will create a ClusterIP service, the default is false;</p>
<p>--port indicates the port exposed by the container. If expose is true, the port is also the service port;</p>
<p>--image specifies the image used by the container;</p>
<p>--dry-run is true, only print the object to be sent, and not send it, the default is false.</p>
<p>Create a deployment named cka-1120-01 with environment variables</p>
<pre><code>kubectl run cka-1120-01 --image=nginx --env=&quot;DNS_DOMAIN=cluster.local&quot; --env=&quot;POD_NAMESPACE=default&quot;
</code></pre>
<p>Create a deployment named cka-1120-02 with label</p>
<pre><code>kubectl run cka-1120-02 --image=nginx --labels=&quot;app=nginx,env=prod&quot;
</code></pre>
<p>There is also a <code>--restart</code> parameter, the default is Always, if set to OnFailure, the job will be created; if set to Never, the ordinary Pod will be created.</p>
<pre><code>kubectl run cka-1120-03 --image=nginx --restart=OnFailure

kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
job.batch/cka-1120-03 created
</code></pre>
<pre><code>kubectl run cka-1120-04 --image=nginx --restart=Never

pod/cka-1120-04 created
</code></pre>
<p><code>--schedule</code> specifies the timing rule of cronjob, if this parameter is specified, cronjob will be created.</p>
<pre><code>kubectl run pi --schedule=&quot;0/5 * * * ?&quot; --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'

cronjob.batch/pi created
</code></pre>
<p>Currently, it is not supported to directly create resource objects such as Statefulset and Daemonset.</p>
<p><strong>What happens after kubectl run is executed?</strong></p>
<p>https://github.com/kubernetes/kubernetes/blob/master/cmd/clicheck/check_cli_conventions.go</p>
<pre><code>func main() {
	var errorCount int

	kubectl := cmd.NewKubectlCommand(os.Stdin, ioutil.Discard, ioutil.Discard)
	errors := cmdsanity.RunCmdChecks(kubectl, cmdsanity.AllCmdChecks, []string{})
	for _, err := range errors {
		errorCount++
		fmt.Fprintf(os.Stderr, &quot;     %d. %s\n&quot;, errorCount, err)
	}
</code></pre>
<p><code>cmd.NewKubectlCommand</code> is to build <code>kubectl</code> and its sub-command line parameters. The final code to execute the business logic is under the &quot;pkg\kubectl&quot; package. Different subcommands: apply, run, create entry corresponding to &quot;pkg\kubectl\cmd&quot;<br>
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go</p>
<pre><code>func NewCmdRun(f cmdutil.Factory, streams genericclioptions.IOStreams) *cobra.Command {
	o := NewRunOptions(streams)

	cmd := &amp;cobra.Command{
		Use:                   &quot;run NAME --image=image [--env=\&quot;key=value\&quot;] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]&quot;,
		DisableFlagsInUseLine: true,
		Short:                 i18n.T(&quot;Run a particular image on the cluster&quot;),
		Long:                  runLong,
		Example:               runExample,
		Run: func(cmd *cobra.Command, args []string) {
			cmdutil.CheckErr(o.Complete(f, cmd))
			cmdutil.CheckErr(o.Run(f, cmd, args))
		},
	}
</code></pre>
<p><code>o.Run(f, cmd, args)</code> will perform a series of checks on the parameters passed by kubectl run, filled with default values</p>
<pre><code>	var createdObjects = []*RunObject{}
	runObject, err := o.createGeneratedObject(f, cmd, generator, names, params, cmdutil.GetFlagString(cmd, &quot;overrides&quot;), namespace)
	if err != nil {
		return err
	}
	createdObjects = append(createdObjects, runObject)

	allErrs := []error{}
	if o.Expose {
		serviceGenerator := cmdutil.GetFlagString(cmd, &quot;service-generator&quot;)
		if len(serviceGenerator) == 0 {
			return cmdutil.UsageErrorf(cmd, &quot;No service generator specified&quot;)
		}
		serviceRunObject, err := o.generateService(f, cmd, serviceGenerator, params, namespace)
		if err != nil {
			allErrs = append(allErrs, err)
		} else {
			createdObjects = append(createdObjects, serviceRunObject)
		}
	}
</code></pre>
<p><code>o.generateService</code> Generate deployment, cronjob, job, pod and other resource objects according to different generators, and send creation requests to apiserver.</p>
<p>If expose is set to true, the same call o.createGeneratedObject is used to generate and create a service.</p>
<pre><code>func (o *RunOptions) createGeneratedObject(f cmdutil.Factory, cmd *cobra.Command, generator generate.Generator, names []generate.GeneratorParam, params map[string]interface{}, overrides, namespace string) (*RunObject, error) {
	err := generate.ValidateParams(names, params)
	if err != nil {
		return nil, err
	}

	// TODO: Validate flag usage against selected generator. More tricky since --expose was added.
	obj, err := generator.Generate(params)
	if err != nil {
		return nil, err
	}
</code></pre>
<p><code>generator.Generate(params)</code> generates different resource objects according to different generator implementations.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 2 | Interfacing CSI storage knowledge]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-2/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-2/">
        </link>
        <updated>2020-04-04T03:19:12.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Under the Kubernetes PVC + PV system, What components need to participate the volume plug-in implemented by CSI, from the pv being dynamically created to pv being used by pods?</p>
<p>A.</p>
<pre><code>PersistentVolumeController + CSI-Provisoner + CSI controller plugin
</code></pre>
<p>B.</p>
<pre><code>AttachDetachController + CSI-Attacher + CSI controller plugin
</code></pre>
<p>C.</p>
<pre><code>Kubelet + CSI node plugin
</code></pre>
<h2 id="answer-a-b-and-c">Answer: A, B and C</h2>
<p>K8s uses <code>PVC</code> to describe the size of the persistent storage that Pod wants to use, read and write permissions, etc., which are generally created by developers; <code>PV</code> describes specific storage types, storage addresses, mounting directories, etc., generally by Ops personnel create. Instead of writing the volume information directly in the pod. First, it can make the development and operation and maintenance responsibilities clear. Second, the use of <code>PVC</code> and <code>PV</code> mechanisms can be well extended to support different storage implementations on the market, such as k8s v1.10 version for Local Persistent Volume.</p>
<p>Let's try to understand the overall process from Pod creation to volume availability.</p>
<p>The user submits a request to create a pod, and <code>PersistentVolumeController</code> finds that the pod claims to use <code>PVC</code>, and it will help it find a <code>PV</code> pair.</p>
<p>If there is no ready-made <code>PV</code>, go to the corresponding <code>StorageClass</code>, help it create a new <code>PV</code>, and then complete the binding with <code>PVC</code>.</p>
<p>The newly created <code>PV</code> is still just an API object, and it needs to undergo &quot;two-stage processing&quot; before it can be used as the &quot;persistent volume&quot; on the host machine:</p>
<ul>
<li>
<p>In the first phase, <code>AttachDetachController</code> running on the master is responsible for attaching the <code>PV</code> and mounting the remote disk for the host;</p>
</li>
<li>
<p>The second stage is to run inside the kubelet component on each node, mount the remote disk attached in the first step to the host directory. This control loop is called <code>VolumeManagerReconciler</code>, which runs on a separate Goroutine and does not block the kubelet main control loop.</p>
</li>
</ul>
<p>After completing these two steps, the &quot;persistent volume&quot; corresponding to the PV is ready, and the POD can be started normally, and the &quot;persistent volume&quot; is mounted on the specified path in the container.</p>
<p>k8s supports writing its own storage plug-in <code>FlexVolume</code> and <code>CSI</code>. Either way, you need to go through &quot;two-stage processing.&quot; FlexVolume has more limitations than CSI. Generally, we use <code>CSI</code> to dock storage.</p>
<p>The design idea of ​​the <code>CSI</code> plug-in system strips this Provision stage (dynamically creating <code>PV</code>) and part of the storage management functions in Kubernetes from the main code to make it into several separate components. These components will monitor changes in storage-related events in Kubernetes, such as the creation of <code>PVC</code>, through the Watch API to perform specific storage management actions.</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586270614137.png" alt="" loading="lazy"></figure>
<p>The three independent external components (External Components) in the CSI storage plug-in system in the above figure, namely: <code>Driver Registrar</code>, <code>External Provisioner</code> and <code>External Attacher</code>, correspond to some storage management functions stripped from the Kubernetes project.</p>
<p>We need to implement a binary of Custom Components, which will provide three services in gRpc mode: <code>CSI Identity</code>, <code>CSI Controller</code>, and <code>CSI Node</code>.</p>
<p><code>Driver Registrar</code> component, which is responsible for registering the plug-in in kubelet; <code>Driver Registrar</code> calls CSI Identity service to obtain plug-in information.</p>
<p>The <code>External Provisioner</code> component listens to the PVC objects in the APIServer. When a PVC is created, it will call the CreateVolume method of the CSI Controller to create the corresponding PV;</p>
<p>The <code>External Attacher</code> component is responsible for the Attach phase. The Mount phase is completed by the <code>VolumeManagerReconciler</code> control loop in the kubelet directly calling the CSI Node service.</p>
<p>After the two stages are completed, kubelet passes the mount parameter to docker to create and start the container.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 1 | Daemonset, docking storage CSI knowledge points]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day1/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day1/">
        </link>
        <updated>2020-04-03T12:52:09.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Which of the following <code>Daemonset</code> yaml are correct:</p>
<p>A.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels: k8s-app: fluentd-logging 
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
        restartPolicy: Never
</code></pre>
<p>B.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata: 
      labels: 
        name: fluentd-elasticsearch
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Onfailure
</code></pre>
<p>C.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Always
</code></pre>
<p>D.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
</code></pre>
<h2 id="answer-c-and-d"><strong>Answer: C and D</strong></h2>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586221558403.jpeg" alt="" loading="lazy"></figure>
<p><code>RestartPolicy</code> field, optional values are <code>Always</code>, <code>OnFailure</code>, and <code>Never</code>. The default is Always. There can be multiple containers in a Pod, and restartPolicy applies to all containers in the Pod. The purpose of restartPolicy is to let kubelet restart the failed container.</p>
<p>In addition, the <code>restartPolicy</code> of <code>Deployment</code> and <code>Statefulset</code> must also be <code>Always</code> to ensure that the pod exits abnormally, or the health check livenessProbe fails and the kubelet restarts the container.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/deployment</p>
<p><code>Job</code> and <code>CronJob</code> are pods that run once, and <code>restartPolicy</code> can only be <code>OnFailure</code> or <code>Never</code>, to ensure that the container will not restart after the execution is completed.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion</p>
]]></content>
    </entry>
</feed>