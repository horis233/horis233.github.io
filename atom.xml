<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jiaming-hu.com</id>
    <title>Jiaming&apos;s Blog</title>
    <updated>2020-05-19T15:32:01.431Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jiaming-hu.com"/>
    <link rel="self" href="https://jiaming-hu.com/atom.xml"/>
    <subtitle>Why’s THE Design</subtitle>
    <logo>https://jiaming-hu.com/images/avatar.png</logo>
    <icon>https://jiaming-hu.com/favicon.ico</icon>
    <rights>All rights reserved 2020, Jiaming&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 11 | analysis and example exercise of kubectl's common commands of the most complete kubernetes]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-12-or-analysis-and-example-exercise-of-kubectls-common-commands-of-the-most-complete-kubernetes/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-12-or-analysis-and-example-exercise-of-kubectls-common-commands-of-the-most-complete-kubernetes/">
        </link>
        <updated>2020-05-02T20:50:00.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create two deployment names: cka-1203-01 and cka-1203-02; Add label: CKA: cka-1203-01 to Pod of cka-1203-01; Pod of cka-1203-02 plus label: CKA: cka-1203-02;</p>
<p>Please use the kubectl command label selector to find out the two deployment s and sort them according to the creation time.</p>
<p>For example:</p>
<p>NAME          READY   UP-TO-DATE   AVAILABLE   AGE<br>
cka-1203-01   1/1     1            1           8m40s<br>
cka-1203-02   1/1     1            1           8m38</p>
<h2 id="answer">Answer</h2>
<p>The following operations are carried out in k8s v1.15.2</p>
<p>To create a deployment with labels:</p>
<pre><code class="language-bash">[root@liabio cka]#  kubectl run cka-1203-01   --image=nginx --labels=&quot;cka=cka-1203-01&quot;
deployment.apps/cka-1203-01 created

[root@liabio cka]#  kubectl run cka-1203-02   --image=nginx --labels=&quot;cka=cka-1203-02&quot;
deployment.apps/cka-1203-02 created
</code></pre>
<p>Query the deploy ment of labels with key cka, and sort by time.</p>
<pre><code class="language-bash">[root@liabio cka]# kubectl get deploy --selector=cka --sort-by=.metadata.creationTimestamp
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
cka-1203-01   1/1     1            1           4m24s
cka-1203-02   1/1     1            1           4m4s
</code></pre>
<h2 id="analysis">Analysis</h2>
<p>kubectl get command reference on the official website: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#get</p>
<pre><code class="language-bash"> get [(-o|--output=)json|yaml|wide|custom-columns=...|custom-columns-file=...|go-template=...|go-template-file=...|jsonpath=...|jsonpath-file=...] (TYPE[.VERSION][.GROUP] [NAME | -l label] | TYPE[.VERSION][.GROUP]/NAME ...) [flags]
</code></pre>
<p>--All namespaces: if specified, lists all objects in the namespace. Even if -- namespace and -- all namespaces are specified at the same time, - namespace is ignored. The default value is false, abbreviated to - A, for example: kubectl get Pod - A;</p>
<p>--Field selector: the selector to be filtered (field query). Supports' = ',' = = 'and'! = '(for example -- field selector key1 = value1, key2 = Value2). The server supports only a limited number of field queries of each type.</p>
<p>--Include uninitialized: if true, the kubectl command applies to uninitialized objects. If set to false, this flag overrides other flags that cause the kubectl command to apply to uninitialized objects, such as' - all '. metadata.initializers for metadata is null and is considered initialized. Default is false</p>
<p>--Label columns: accepts a comma separated list of labels that will be displayed as columns. Names are case sensitive. You can also use multiple flag options, short for - L, such as - L label1 -L label2</p>
<pre><code>[root@liabio ~]# kubectl get deploy -L A=A -L b=b
NAME              READY   UP-TO-DATE   AVAILABLE   AGE     A=A   B=B
cka-1203-01       1/1     1            1           7h22m
</code></pre>
<p>--No headers: when using the default or custom column output format, the title will not be printed (the default print title). Default false</p>
<pre><code>[root@liabio ~]# kubectl get deployments. --no-headers 
cka-1203-01       1/1   1     1     7h23m
</code></pre>
<p>--Output: short for - o, output format. One of the following: JSON | yaml | wide | name | custom columns =... | custom columns file =... | go template =... | go template file =... | jsonpath =... | jsonpath file = See custom columns http://kubernetes.io/docs/user-guide/kubectl-overview/#custom-columns golang template: http://golang.org/pkg/text/template/#pkg-overview And jsonpath templates: http://kubernetes.io/docs/user-guide/jsonpath</p>
<p>--Selector: abbreviated to - l label selector (label query) for filtering. It supports' = ',' = ', and'! = '. For example: - l key1=value1,key2=value2. If only key is written, the list of keys in all labels will be listed. If - l=cka, cka=cka-1203-01 and cka=cka-1203-02 will be returned</p>
<p>--Show kind: if it exists, the resource type of the requested object will be listed. The default is false;</p>
<pre><code>[root@liabio ~]# kubectl get deployments. --show-kind
NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.extensions/cka-1203-01       1/1     1            1           7h30m
</code></pre>
<p>--Show labels: the default is false. When printing, all labels will be displayed as the last column (the default is hidden label column)</p>
<pre><code>[root@liabio ~]# kubectl get deployments.  -l=cka --show-labels 
NAME          READY   UP-TO-DATE   AVAILABLE   AGE     LABELS
cka-1203-01   1/1     1            1           7h34m   cka=cka-1203-01
cka-1203-02   1/1     1            1           7h34m   cka=cka-1203-02
</code></pre>
<p>--Sort by: if the parameter is not empty, use this field specification to sort the list. The field specification is represented as a JSONPath expression (for example '{. metadata.name}'). The fields in the API resource specified by this JSONPath expression must be integers or strings.</p>
<p>--watch: short for - w, default is false. After the requested object is listed / obtained, it will be printed when listening for changes. If no object name is specified, uninitialized objects are excluded.</p>
<p>--Watch only: the default is false, which monitors changes to the requested object without first listing / get at the beginning.</p>
<h2 id="example">Example</h2>
<ol>
<li>Query the deploy ment with labels a=a and b=b:</li>
</ol>
<pre><code>[root@liabio cka]#  kubectl run cka-1203-03   --image=nginx --labels=&quot;a=a,b=b&quot;
deployment.apps/cka-1203-03 created

[root@liabio cka]# kubectl get deploy --selector=a=a,b=b
NAME          READY   UP-TO-DATE   AVAILABLE   AGE
cka-1203-03   1/1     1            1           18s
</code></pre>
<ol start="2">
<li>List all PVs in the environment and sort them by name field (using kubectl's own sorting function), list all PVs in the environment and sort them by capacity field</li>
</ol>
<pre><code>kubectl get pv --sort-by=.metadata.name
kubectl get pv --sort-by=.spec.capacity.storage
</code></pre>
<ol start="3">
<li>Lists the number of node s without a label. Pay attention not to add title display</li>
</ol>
<pre><code>kubectl get nodes --selector=role!=staging --no-headers | wc -l 
</code></pre>
<ol start="4">
<li>List the names of all pod s represented by a service under a namepsace.</li>
</ol>
<p>Notice that it's just the name. First kubectl get service to see what the label is, then kubectl get pod -l to see the following names, and then the names that need to be kept are kubectl get pods &quot;-o=custom-columns=NAME:.metadata.name&quot;</p>
<pre><code># Get label
kubectl get svc -o wide
[root@liabio ~]# kubectl get svc -o wide
NAME              TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)           AGE     SELECTOR
solo              ClusterIP   10.99.151.11     &lt;none&gt;        8080/TCP          48d     name=solo
[root@liabio ~]# kubectl get pods -l name=solo -o name
pod/solo-57944994c5-zs5cw
</code></pre>
<ol start="5">
<li>Please customize the column output to output only namespace and pod names.</li>
</ol>
<pre><code>kubectl get pods  -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace
</code></pre>
<p>Using custom columns file</p>
<pre><code>[root@liabio cka]# cat tmp.txt 
NAME          NAMESPACE
metadata.name metadata.namespace
[root@liabio cka]# kubectl get pods  -o custom-columns-file=tmp.txt
NAME                               NAMESPACE
cka-1203-01-849574cf7-b8dtc        default
cka-1203-02-77585c9945-vmb2h       default
cka-1203-03-587d78ccdb-l8n5x       default
</code></pre>
<p>For more examples, please refer to the official website kubectl command example document: https://kubernetes.io/docs/reference/kubectl/cheatsheet/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 10 | k8s access control RBAC, Role, RoleBinding, and leads to kubectl common commands]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-10-or-k8s-access-control-rbac-role-rolebinding-and-leads-to-kubectl-common-commands/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-10-or-k8s-access-control-rbac-role-rolebinding-and-leads-to-kubectl-common-commands/">
        </link>
        <updated>2020-04-20T13:21:26.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create a Role (only all operation permissions of pods in the cka namespace) and RoleBinding (use serviceaccount authentication and authentication), use the corresponding serviceaccount as authentication information to operate on pods in the cka namespace and operations on pods in the default namespace.</p>
<p>– The names of Role and RoleBinding are cka-1202-role, cka-1202-rb<br>
Note: Please attach the complete yaml of the used command, the created Role, RoleBinding and serviceaccount, which can be divided into multiple comments.</p>
<h2 id="answer">Answer</h2>
<p>To create a Service Account:</p>
<pre><code class="language-yaml">[root@liabio cka]# kubectl create serviceaccount cka-1202-sa -n cka  -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: &quot;2019-12-02T23:37:42Z&quot;
  name: cka-1202-sa
  namespace: cka
  resourceVersion: &quot;15159020&quot;
  selfLink: /api/v1/namespaces/cka/serviceaccounts/cka-1202-sa
  uid: 6764e90c-cb28-4de1-9109-6e3d56941fcb
</code></pre>
<p>Create Role:</p>
<pre><code class="language-yaml">[root@liabio cka]# kubectl create role  cka-1202-role -n cka  --verb=* --resource=pods -oyaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: &quot;2019-12-02T23:40:26Z&quot;
  name: cka-1202-role
  namespace: cka
  resourceVersion: &quot;15159247&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/cka/roles/cka-1202-role
  uid: fc2c5593-2fd9-46d7-a809-99bcee32249e
rules:
- apiGroups:
  - &quot;&quot;
  resources:
  - pods
  verbs:
  - '*'
</code></pre>
<p>To create RoleBinding:</p>
<pre><code class="language-yaml">[root@liabio cka]# kubectl create rolebinding cka-1202-rb -n cka  --role=cka-1202-role --serviceaccount=cka:cka-1202-sa  -oyaml 
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: &quot;2019-12-02T23:46:50Z&quot;
  name: cka-1202-rb
  namespace: cka
  resourceVersion: &quot;15159794&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/cka/rolebindings/cka-1202-rb
  uid: c00d104e-a531-4781-90f4-2821651492bf
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cka-1202-role
subjects:
- kind: ServiceAccount
  name: cka-1202-sa
  namespace: cka
</code></pre>
<p>Verification:</p>
<p>Get the secret of Service Account binding cka-1202-sa and base64-d decode the token field:</p>
<pre><code class="language-shell">[root@liabio ~]# kubectl get secret -n cka   
NAME                      TYPE                                  DATA   AGE
cka-1202-sa-token-9rgp4   kubernetes.io/service-account-token   3      42m
default-token-r77xn       kubernetes.io/service-account-token   3      4d14h
[root@liabio ~]# kubectl get secret -n cka  cka-1202-sa-token-9rgp4 -ojson | jq .data.token 
&quot;ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjM0JoWTJVaU9pSmphMkVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxZM0psZEM1dVlXMWxJam9pWTJ0aExURXlNREl0YzJFdGRHOXJaVzR0T1hKbmNEUWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzV1WVcxbElqb2lZMnRoTFRFeU1ESXRjMkVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUkyTnpZMFpUa3dZeTFqWWpJNExUUmtaVEV0T1RFd09TMDJaVE5rTlRZNU5ERm1ZMklpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNlkydGhPbU5yWVMweE1qQXlMWE5oSW4wLnFXanJUcTdEbVZTU01TM0h4YzR0bFd4ODdUNGtvUkNvVmkxMjVzZXNWRWJ2QUtEaTJ6MFhvNjJaNzAza2htQ1dsWTU1TkxPYWVKS2taWXhYOWZMTEdYMnpPVWVFdzFvbUpmRkZpTm41NGxjOUhRTjlRXzVmTjRyYS1WNFZSaU5uQkFUeW43Yzc2aGk2Nks1aUh5WjB4bFRNcnBNQThXN1l2TmJnU1pIOXhnaFdSenpkSElKYWF1UXBTY0xtSk5MNmxGNGd5ZG9Xd0dDQy1QU0VjdGpKTkRtMF8zSTZoUkhEZkJzd3k2d0t4VGx4T3lIdE9yeUc0ckUzZzVqUWZOdV9BNTdTNVlocmEwWVM0emM0X0RvdXBmUC1zVjU3R0FQS1JxODZsRGdlOHo4cWFIaDRyb0k3RTNJbC1DRU9HS1JJeE52SWZVX3d0aHRrMG95aW5HR2wydw==&quot;
[root@liabio ~]# echo ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklpSjkuZXlKcGMzTWlPaUpyZFdKbGNtNWxkR1Z6TDNObGNuWnBZMlZoWTJOdmRXNTBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5dVlXMWxjM0JoWTJVaU9pSmphMkVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxZM0psZEM1dVlXMWxJam9pWTJ0aExURXlNREl0YzJFdGRHOXJaVzR0T1hKbmNEUWlMQ0pyZFdKbGNtNWxkR1Z6TG1sdkwzTmxjblpwWTJWaFkyTnZkVzUwTDNObGNuWnBZMlV0WVdOamIzVnVkQzV1WVcxbElqb2lZMnRoTFRFeU1ESXRjMkVpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1MWFXUWlPaUkyTnpZMFpUa3dZeTFqWWpJNExUUmtaVEV0T1RFd09TMDJaVE5rTlRZNU5ERm1ZMklpTENKemRXSWlPaUp6ZVhOMFpXMDZjMlZ5ZG1salpXRmpZMjkxYm5RNlkydGhPbU5yWVMweE1qQXlMWE5oSW4wLnFXanJUcTdEbVZTU01TM0h4YzR0bFd4ODdUNGtvUkNvVmkxMjVzZXNWRWJ2QUtEaTJ6MFhvNjJaNzAza2htQ1dsWTU1TkxPYWVKS2taWXhYOWZMTEdYMnpPVWVFdzFvbUpmRkZpTm41NGxjOUhRTjlRXzVmTjRyYS1WNFZSaU5uQkFUeW43Yzc2aGk2Nks1aUh5WjB4bFRNcnBNQThXN1l2TmJnU1pIOXhnaFdSenpkSElKYWF1UXBTY0xtSk5MNmxGNGd5ZG9Xd0dDQy1QU0VjdGpKTkRtMF8zSTZoUkhEZkJzd3k2d0t4VGx4T3lIdE9yeUc0ckUzZzVqUWZOdV9BNTdTNVlocmEwWVM0emM0X0RvdXBmUC1zVjU3R0FQS1JxODZsRGdlOHo4cWFIaDRyb0k3RTNJbC1DRU9HS1JJeE52SWZVX3d0aHRrMG95aW5HR2wydw== | base64 -d
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJja2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiY2thLTEyMDItc2EtdG9rZW4tOXJncDQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY2thLTEyMDItc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2NzY0ZTkwYy1jYjI4LTRkZTEtOTEwOS02ZTNkNTY5NDFmY2IiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6Y2thOmNrYS0xMjAyLXNhIn0.qWjrTq7DmVSSMS3Hxc4tlWx87T4koRCoVi125sesVEbvAKDi2z0Xo62Z703khmCWlY55NLOaeJKkZYxX9fLLGX2zOUeEw1omJfFFiNn54lc9HQN9Q_5fN4ra-V4VRiNnBATyn7c76hi66K5iHyZ0xlTMrpMA8W7YvNbgSZH9xghWRzzdHIJaauQpScLmJNL6lF4gydoWwGCC-PSEctjJNDm0_3I6hRHDfBswy6wKxTlxOyHtOryG4rE3g5jQfNu_A57S5Yhra0YS4zc4_DoupfP-sV57GAPKRq86lDge8z8qaHh4roI7E3Il-CEOGKRIxNvIfU_wthtk0oyinGGl2w[root@liabio ~]#
</code></pre>
<p>Add the decoded information to ~ /. kube/config, and notice that the context with name as codeaction and the user with name as codeaction are added below</p>
<pre><code class="language-yaml">apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDLQo=
    server: https://10.0.0.0:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: coderaction
  name: coderaction
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: coderaction
  user:
    token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJja2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoiY2thLTEyMDItc2EtdG9rZW4tOXJncDQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiY2thLTEyMDItc2EiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiI2NzY0ZTkwYy1jYjI4LTRkZTEtOTEwOS02ZTNkNTY5NDFmY2IiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6Y2thOmNrYS0xMjAyLXNhIn0.qWjrTq7DmVSSMS3Hxc4tlWx87T4koRCoVi125sesVEbvAKDi2z0Xo62Z703khmCWlY55NLOaeJKkZYxX9fLLGX2zOUeEw1omJfFFiNn54lc9HQN9Q_5fN4ra-V4VRiNnBATyn7c76hi66K5iHyZ0xlTMrpMA8W7YvNbgSZH9xghWRzzdHIJaauQpScLmJNL6lF4gydoWwGCC-PSEctjJNDm0_3I6hRHDfBswy6wKxTlxOyHtOryG4rE3g5jQfNu_A57S5Yhra0YS4zc4_DoupfP-sV57GAPKRq86lDge8z8qaHh4roI7E3Il-CEOGKRIxNvIfU_wthtk0oyinGGl2w
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1CRUdJTiB1M1Y2NDTnpPUT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
    client-key-data: LS0tLS1CBS0NBUUVBdjNpTkx5eUEwaVdmOU1hUjA3cVFTOEtFWS0tLS0tCg==
</code></pre>
<p>By switching to the use context of coding, you can find that when you get the Pod under the default partition, you will be prompted that system:serviceaccount:cka:cka-1202-sa does not have permission, but you can get the Pods under the cka namespace normally</p>
<pre><code class="language-shell">[root@liabio cka]# kubectl config use-context kubernetes-admin@kubernetes
Switched to context &quot;kubernetes-admin@kubernetes&quot;.
[root@liabio cka]# kubectl get pod
NAME                               READY   STATUS    RESTARTS   AGE
cka-1128-01-7b8b8cb79-mll6d        1/1     Running   118        32h
[root@liabio cka]# 
[root@liabio cka]# 
[root@liabio cka]# kubectl get node
NAME     STATUS   ROLES    AGE    VERSION
liabio   Ready    master   141d   v1.15.2
[root@liabio cka]# kubectl config use-context coderaction
Switched to context &quot;coderaction&quot;.
[root@liabio cka]# kubectl get pod
Error from server (Forbidden): pods is forbidden: User &quot;system:serviceaccount:cka:cka-1202-sa&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;default&quot;
[root@liabio cka]# kubectl get pod -n cka
No resources found.
</code></pre>
<h2 id="analysis">Analysis</h2>
<p>k8s provides two security steps for accessing API: authentication and authorization. Authentication solves the problem of who the user is, and authorization solves the problem of what the user can do. Through reasonable authority management, it can ensure the security and reliability of the system.</p>
<p>All operations of k8s cluster are basically carried out through Kube API server, which provides HTTP RESTful API for clients inside and outside the cluster to call. It should be noted that the authentication and authorization process only exists in the API in the form of HTTPS. In other words, if the client uses HTTP to connect to the Kube API server, the authentication and authorization will not be performed. Therefore, it can be set as follows: http is used for communication between components inside the cluster, and HTTPS is used outside the cluster, which not only increases security, but also is not too complex.</p>
<p>This paper mainly studies authorization: RBAC.</p>
<p>RBAC official documents: https://kubernetes.io/docs/reference/access-authn-authz/rbac/</p>
<p>Instructions for creating RoleBinding, Role, and Service Account official websites: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-rolebinding-em-</p>
<p>Use the kubeconfig file to organize cluster access: https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/</p>
<p>Official command guide for context related operations: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#config</p>
<p>Role based access control (RBAC) is a method to adjust access to computer or network resources based on the roles of users in the enterprise.</p>
<p>RBAC uses the rbac.authorization.k8s.io API Group to drive authorization decisions, enabling administrators to dynamically configure policies through the Kubernetes API.</p>
<p>Starting from 1.8, RBAC mode is stable and supported by rbac.authorization.k8s.io/v1 API.</p>
<p>To enable RBAC, start apiserver -- authorization mode = RBAC</p>
<p>The RBAC API declares four top-level types:</p>
<p>Role and ClusterRole<br>
In the RBAC API, a Role contains rules that represent a set of permissions. Permissions are purely cumulative (no &quot;deny&quot; rule). You can use Role in namespace or ClusterRole in cluster scope.</p>
<p>Role can only be used to grant access to resources within a single namespace.</p>
<p>Clusterroles because they are cluster wide, they can also be used to grant the following permissions:</p>
<p>Cluster wide resources (such as nodes)<br>
Non resource endpoint (for example &quot;/ health Hz&quot;)<br>
namespace resources (such as pod) in all namespaces<br>
RoleBinding and ClusterRoleBinding<br>
RoleBinding grants permissions defined in Role to one or a Group of users. It contains subjects (User, Group, or Service Account) and references to the granted roles. You can use RoleBinding in namespace or ClusterRoleBinding in cluster scope.</p>
<p>Role binding can refer to roles under the same namespace.</p>
<p>roleRef is how the binding is actually created. The kind can be Role or ClusterRole, and name will refer to the Role or ClusterRole with specific name</p>
<p>ClusterRoleBinding can grant permissions at the cluster level and in all namespace s.</p>
<p>Create Role command:</p>
<pre><code class="language-shell">kubectl create role NAME --verb=verb --resource=resource.group/subresource [--resource-name=resourcename] [--dry-run]
</code></pre>
<p>--verb specifies the collection of operation actions for resources, including get, delete, update, create, patch, watch, and list. All operation actions are* --Resource specifies the set of actionable resource types; --Resource name specifies the set of actionable resource names; Such as:</p>
<pre><code class="language-shell">[root@liabio ~]# kubectl create role pod-reader-cka -n cka  --verb=get --verb=list --resource=pods --resource-name=readablepod --resource-name=anotherpod -oyaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: &quot;2019-12-03T03:50:34Z&quot;
  name: pod-reader-cka
  namespace: cka
  resourceVersion: &quot;15179947&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/cka/roles/pod-reader-cka
  uid: 16742721-4890-43de-9725-d6c721c6e4cf
rules:
- apiGroups:
  - &quot;&quot;
  resourceNames:
  - readablepod
  - anotherpod
  resources:
  - pods
  verbs:
  - get
  - list
</code></pre>
<p>Create RoleBinding</p>
<pre><code class="language-shell">kubectl create rolebinding NAME --clusterrole=NAME|--role=NAME [--user=username] [--group=groupname] [--serviceaccount=namespace:serviceaccountname] [--dry-run]
</code></pre>
<p>--Role specifies the role name in roleRef of RoleBinding; --ClusterRole specifies the ClusterRole name in roleRef of RoleBinding; --serviceaccount specifies the subjects collection of RoleBinding; --User specifies the name of user under subjects of RoleBinding; Such as:</p>
<pre><code class="language-shell">[root@liabio ~]# kubectl create rolebinding admin-cka -n cka --clusterrole=admin --user=user1 --user=user2 --group=group1 -oyaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: &quot;2019-12-03T03:47:55Z&quot;
  name: admin-cka
  namespace: cka
  resourceVersion: &quot;15179732&quot;
  selfLink: /apis/rbac.authorization.k8s.io/v1/namespaces/cka/rolebindings/admin-cka
  uid: 4d4eacfb-3ba0-4fa1-96c3-c624fbafb12c
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user1
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: user2
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: group1
</code></pre>
<p>Create ServiceAccount<br>
kubectl create serviceaccount NAME [--dry-run]<br>
Such as:</p>
<pre><code class="language-shell">[root@liabio cka]# kubectl create serviceaccount cka-1202-sa -n cka  -o yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: &quot;2019-12-02T23:37:42Z&quot;
  name: cka-1202-sa
  namespace: cka
  resourceVersion: &quot;15159020&quot;
  selfLink: /api/v1/namespaces/cka/serviceaccounts/cka-1202-sa
  uid: 6764e90c-cb28-4de1-9109-6e3d56941fcb
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Play with Openshift 4 -- add admin user]]></title>
        <id>https://jiaming-hu.com/post/play-with-openshift-4-add-admin-user/</id>
        <link href="https://jiaming-hu.com/post/play-with-openshift-4-add-admin-user/">
        </link>
        <updated>2020-04-18T20:49:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="important">Important</h2>
<p>This article is mainly applicable to use HTPasswd Identity Provider mechanism to verify username and password.<br>
Check whether OpenShift 4 has an HTPasswd Identity Provider (IDP).</p>
<pre><code class="language-bash">$ oc get identity
NAME IDP NAME IDP USER NAME USER NAME USER UID
htpasswd_provider: admin htpasswd_provider admin admin e26157a0-1903-11ea-a73b-0a580a8000c9
</code></pre>
<p>If there is no HTPasswd Identity Provider in OpenShift 4 OAuth, you can refer to &quot;Play with Openshift 4 -- add HTPasswd authentication&quot; Creating an HTPasswd Identity Provider and adding users.<br>
Make sure to install httpd-tools.</p>
<pre><code class="language-bash">$ sudo yum -y install httpd-tools
</code></pre>
<p>Add administrator user<br>
After installing the CodeReady Container based on OpenShift 4, there are only two users, kubeadmin and developer. We can increase the number of logged-in users through the following methods and give administrator rights.</p>
<ol>
<li>Log in with kubeadmin (that is, cluster administrator user), and then execute the command to obtain the Name of Htpasswd of Identity Providers (that is, htpass-secret in the following)</li>
</ol>
<pre><code class="language-yaml">[root@jiaming-inf ~]# oc describe oauth cluster
Name:         cluster
Namespace:    
Labels:       &lt;none&gt;
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {&quot;apiVersion&quot;:&quot;config.openshift.io/v1&quot;,&quot;kind&quot;:&quot;OAuth&quot;,&quot;metadata&quot;:{&quot;annotations&quot;:{},&quot;name&quot;:&quot;cluster&quot;},&quot;spec&quot;:{&quot;identityProviders&quot;:[{&quot;challe...
              release.openshift.io/create-only: true
API Version:  config.openshift.io/v1
Kind:         OAuth
Metadata:
  Creation Timestamp:  2020-04-17T23:20:45Z
  Generation:          2
  Resource Version:    1174472
  Self Link:           /apis/config.openshift.io/v1/oauths/cluster
  UID:                 81f9920e-416e-43bd-95eb-3c0ceac061c5
Spec:
  Identity Providers:
    Htpasswd:
      File Data:
        Name:        htpass-secret
    Mapping Method:  claim
    Name:            my_htpasswd_provider
    Type:            HTPasswd
Events:              &lt;none&gt;
</code></pre>
<ol start="2">
<li>Export the user and password included in the htpass-secret in the existing OpenShift environment.</li>
</ol>
<pre><code class="language-bash">$ oc get secret htpass-secret -ojsonpath = {. data.htpasswd} -n openshift-config | base64 -d | tee htpasswd.txt
</code></pre>
<ol start="3">
<li>Execute the command to add a user (admin / admin) to the file. Note: Each user is on a separate line when confirming the contents of the file! I encountered a situation where there was no line break when appending the first user, which can be solved manually.</li>
</ol>
<pre><code class="language-bash">$ htpasswd -b htpasswd.txt admin admin
</code></pre>
<ol start="4">
<li>Adding password for user admin</li>
</ol>
<pre><code class="language-bash">[root@jiaming-inf ~]# cat htpasswd.txt 
admin:$apr1$uitiuQon$TiX28LEYKUMuTND48V2711
</code></pre>
<ol start="5">
<li>Import the user file password file into OpenShift.</li>
</ol>
<pre><code class="language-bash">$ oc create secret generic htpass-secret --from-file = htpasswd = htpasswd.txt --dry-run -o yaml -n openshift-config | oc replace -f-
</code></pre>
<p>At this time, you can also enter the openshift-config project in the OpenShift Console, then find htpass-secret from Workloads-&gt; Secrets, and then click Reveal Values ​​in the Data area to view the user name and encrypted password<br>
<img src="https://jiaming-hu.com/post-images/1587329980463.png" alt="" loading="lazy"></p>
<ol start="6">
<li>Use kubeadmin to assign &quot;cluster administrator&quot; permissions to the newly created user.</li>
</ol>
<pre><code class="language-bash">$ oc adm policy add-cluster-role-to-user cluster-admin admin
</code></pre>
<ol start="7">
<li>Use the new user to log in. Note: There may be a login failure (401 Unauthorized) when you log in immediately after creating the user. Wait a few minutes and execute it (this is because OpenShift converts the htpass-secret user to the OpenShift User object).</li>
</ol>
<pre><code class="language-bash">$ oc login -u admin -p admin
</code></pre>
<ol start="8">
<li>Run the command to view the existing OpenShift User object (not an existing valid user. The difference between &quot;User&quot; and &quot;User Object&quot; is that if the user has never logged in to OpenShift, there will be no User object. After being deleted, the User object will not be deleted automatically).</li>
</ol>
<pre><code class="language-yaml">$ oc get users
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Play with Openshift 4 -- add HTPasswd authentication]]></title>
        <id>https://jiaming-hu.com/post/play-with-openshift-4-add-htpasswd-authentication/</id>
        <link href="https://jiaming-hu.com/post/play-with-openshift-4-add-htpasswd-authentication/">
        </link>
        <updated>2020-04-18T20:12:49.000Z</updated>
        <content type="html"><![CDATA[<h2 id="openshift-authentication-mechanism">OpenShift authentication mechanism</h2>
<p>OpenShift implements user identity authentication through OAuth. Through the following Identity Provider, OpenShift 4 supports multiple user identity authentication methods:</p>
<ul>
<li>HTPasswd (HTPasswd is a simplest user identity storage authentication mechanism, which directly puts the user name and encrypted password directly in a text file)</li>
<li>Keystone</li>
<li>LDAP</li>
<li>Basic authentication</li>
<li>Request header</li>
<li>GitHub or GitHub Enterprise</li>
<li>GitLab</li>
<li>Google</li>
<li>OpenID Connect</li>
</ul>
<h2 id="add-htpasswd-identity-provider">Add HTPasswd Identity Provider</h2>
<p>After installing OpenShift 4, we can log in as the &quot;administrator&quot; user and then login to the console.</p>
<p>If OpenShift has not configured the Identity Provider in HTPasswd mode, you can use the following methods to add.</p>
<ol>
<li>Create a Custom Resource file that defines HTPasswd Identity Provider. The name of the HTPasswd Identity Provider is &quot;my_htpasswd_provider&quot;, and OAuth uses a secret named &quot;htpass-secret&quot; to verify the username and password (in the following chapters, a &quot;users.htpasswd&quot; file will be created and then loaded into htpass-secret).</li>
</ol>
<pre><code class="language-yaml">apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_htpasswd_provider 
    challenge: true 
    login: true 
    mappingMethod: claim 
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret 
</code></pre>
<ol start="2">
<li>Create HTPasswd Identity Provider named my_htpasswd_provider according to the htpassed-cr.yaml file.</li>
</ol>
<pre><code class="language-bash">oc apply -f htpassed-cr.yaml
</code></pre>
<h2 id="create-user-by-htpasswd">Create user by HTPasswd</h2>
<ol>
<li>Install httpd-tools<pre><code class="language-bash">yum install httpd-tools
</code></pre>
</li>
<li>Write username (admin) and password (admin) into <code>users.htpasswd</code> file.<pre><code class="language-bash">htpasswd -c -B -b users.htpasswd admin admin
</code></pre>
</li>
<li>You can check users.htpasswd, admin password has been encrypted<pre><code class="language-bash">cat users.htpasswd
</code></pre>
</li>
</ol>
<h2 id="add-usershtpasswd-data-into-secret">Add users.htpasswd data into secret</h2>
<p>OpenShift uses the Secret object to save encrypted data, so we need to load the users and passwords in the users.htpasswd file into the Secret object so that my_htpasswd_provider can use them for identity authentication.</p>
<ol>
<li>Load users.htpasswd into the secret object named htpass-secret.</li>
</ol>
<pre><code class="language-bash">$ oc create secret generic htpass-secret --from-file = htpasswd = users.htpasswd -n openshift-config
</code></pre>
<ol start="2">
<li>You can also log in to the OpenShift Console, enter the openshift-config project, and then find htpass-secret, you can view the user information it loads.</li>
</ol>
<h2 id="verification">verification</h2>
<p>Finally, log in and verify with admin / admin.</p>
<pre><code class="language-bash">oc login -u admin -p admin
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 9 | (isolated from the network) to allow access to A B, C is not allowed to access B, how to do it]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-9-or-isolated-from-the-network-to-allow-access-to-a-b-c-is-not-allowed-to-access-b-how-to-do-it-k8s-access-control-rbac-role-rolebinding-and-lead-serviceaccount/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-9-or-isolated-from-the-network-to-allow-access-to-a-b-c-is-not-allowed-to-access-b-how-to-do-it-k8s-access-control-rbac-role-rolebinding-and-lead-serviceaccount/">
        </link>
        <updated>2020-04-16T20:24:44.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Deployment application deployment three (A, B, C), to allow access to B application A, application B but not allowed to access C.</p>
<ul>
<li>Deployment name for cka-1128-01, cka-1128-02, cka-1128-03</li>
<li>Network Policy name for cka-1128-np</li>
</ul>
<p><strong>Note:</strong> The command to create a complete deployment and network policy yaml, B and prove that A can access the application; C does not allow access B application. Divided into several comments.</p>
<h2 id="answer">Answer</h2>
<p>The first Deploy file cka-1128-01.yaml, using a radial/busyboxplusmirror because there are no curl busybox commands.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-01
spec:
  selector:
    matchLabels:
      app: cka-1128-01
  template:
    metadata:
      labels:
        app: cka-1128-01
    spec:
      containers:
        - name: cka-1128-01
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>CKA-1128-02.yaml:</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-02
spec:
  selector:
    matchLabels:
      app: cka-1128-02
  template:
    metadata:
      labels:
        app: cka-1128-02
    spec:
      containers:
        - name: cka-1128-02
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>CKA-1128-03.yaml:</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cka-1128-03
spec:
  selector:
    matchLabels:
      app: cka-1128-03
  template:
    metadata:
      labels:
        app: cka-1128-03
    spec:
      containers:
        - name: cka-1128-03
          image: radial/busyboxplus
          command: ['sh', '-c', 'sleep 1000']
          imagePullPolicy: IfNotPresent
</code></pre>
<p>You can see A, C can access the B:</p>
<pre><code class="language-bash">[root@liabio cka]# kubectl get pod -owide | grep cka
cka-1128-01-7b8b8cb79-mll6d        1/1     Running   0          3m5s   192.168.155.124   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-02-69dd65bdb7-mfq26       1/1     Running   0          3m8s   192.168.155.117   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-03-66f8f69ff-64q75        1/1     Running   0          3m3s   192.168.155.116   liabio   &lt;none&gt;           &lt;none&gt;
[root@liabio cka]# kubectl exec -ti cka-1128-01-7b8b8cb79-mll6d  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.146 ms
64 bytes from 192.168.155.117: seq=1 ttl=63 time=0.095 ms
[root@liabio cka]# kubectl exec -ti cka-1128-03-66f8f69ff-64q75  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.209 ms
64 bytes from 192.168.155.117: seq=1 ttl=63 time=0.112 ms
</code></pre>
<p>New cka-1128-np.yaml, <code>kubectl apply -f cka-1128-np.yaml</code> creating <code>Network Policy</code>, spec.podSelector.matchLabels select Pod B management; ingress.from.podSelector.matchLabels only specify the flow rate from the opening A whitelist.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: cka-1128-np
spec:
  podSelector:
    matchLabels:
      app: cka-1128-02
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: cka-1128-01
</code></pre>
<p>A verification discovery can ping B, C can not ping B.</p>
<pre><code class="language-bash">[root@liabio cka]# kubectl apply -f cka-1128-np.yaml 
networkpolicy.networking.k8s.io/cka-1128-np created
[root@liabio cka]# kubectl get networkpolicies.
NAME          POD-SELECTOR      AGE
cka-1128-np   app=cka-1128-02   13s
[root@liabio cka]# 
[root@liabio cka]# kubectl get pod -owide | grep cka
cka-1128-01-7b8b8cb79-mll6d        1/1     Running   1          24m    192.168.155.124   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-02-69dd65bdb7-mfq26       1/1     Running   1          24m    192.168.155.117   liabio   &lt;none&gt;           &lt;none&gt;
cka-1128-03-66f8f69ff-64q75        1/1     Running   1          24m    192.168.155.116   liabio   &lt;none&gt;           &lt;none&gt;
[root@liabio cka]# kubectl exec -ti cka-1128-01-7b8b8cb79-mll6d  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
64 bytes from 192.168.155.117: seq=0 ttl=63 time=0.213 ms
[root@liabio cka]# kubectl exec -ti cka-1128-03-66f8f69ff-64q75  -- ping 192.168.155.117
PING 192.168.155.117 (192.168.155.117): 56 data bytes
......
</code></pre>
<h2 id="analysis">Analysis</h2>
<p>The key point of this question is to examine k8s network policy NetworkPolicy, this knowledge is also often asked in the interview. The following excerpt from the official document:</p>
<h3 id="concept">Concept</h3>
<p>Network Policy (NetworkPolicy) is a specification about communication between the pod and pod and other rules between network endpoints are allowed.<br>
NetworkPolicy tag selector resource pod, and the pod is allowed select a definition of communication rules. Network policy implemented by the network plug-in, so users must use a network solution supports the NetworkPolicy,</p>
<p>Network policy concept with the introduction of the official document:<br>
https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource</p>
<p>Network policy practice official documents, this title can refer to the documentation is complete.<br>
https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/</p>
<h3 id="premise">premise</h3>
<p>Network policy implemented by the network plug-in, so users must use a network solution supports NetworkPolicy of - simply create resource objects, but no controller to take effect, then, there is no effect.</p>
<p>###Supported network solutions<br>
Calico、 Kube-router、 Cilium、Romana、Weave Net</p>
<p>k8s official documentation:<br>
https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/</p>
<h3 id="isolated-and-non-isolated-pod">Isolated and non-isolated Pod</h3>
<p>By default, Pod non-isolated, they receive from any source.</p>
<p>Pod can be isolated by the relevant network policy. Once the namespace has NetworkPolicy selected a specific Pod, the Pod will refuse to connect to the network strategy not allowed. (Other network policy is not selected Pod will continue to receive all the traffic in the namespace)</p>
<h3 id="networkpolicy-resources">NetworkPolicy resources</h3>
<p>Here is an example of a NetworkPolicy:</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
</code></pre>
<p>Unless the Select Network solution supports NetworkPolicy, otherwise it will be sent to the above example APIServer no effect.</p>
<p>spec: NetworkPolicy spec contains all the information needed to define a specific network policy in a namespace</p>
<p>podSelector: Each NetworkPolicy comprises a podSelector, it is selected the group policy applies Pod. Because NetworkPolicy currently only supports ingress define rules for the policy is the definition of &quot;target pod&quot; on podSelector essence here. Strategies example with pod &quot;role = db&quot; tag. Empty podSelector select all pod under the namespace.</p>
<p>policyTypes : Each NetworkPolicy policyTypes contains a list, which may contain Ingress, Egress or both. policyTypes field indicates whether a given policy applies to inbound traffic of the selected Pod, Pod outbound traffic from a selected, or both are applicable. If you do not specify any policyType on NetworkPolicy, the default is always set Ingress, and if NetworkPolicy have any export rules, then the Egress settings.</p>
<p>ingress: List each NetworkPolicy ingress rule contains a white list. (Them) from the rule matching to allow simultaneous flow ports and portions. Example policy includes a simple rule: it matches a single Port, from one of two sources, the first specified by namespaceSelector, designated by the second podSelector.</p>
<p>egress: a list of every NetworkPolicy egress rule contains a white list. Each rule allowed to match and flow port section. The example policy contains a rule that matches the traffic on a single port to any destination of 10.0.0.0/24.</p>
<p>So, the example network policy:</p>
<p>Under isolation &quot;default&quot; namespace &quot;role = db&quot; of the pod (if they are not already isolated words).<br>
Allow pod with the &quot;role = frontend&quot; tag from the &quot;default&quot; namespace 6379 is connected to the TCP port of the pod &quot;default&quot; in the namespace.</p>
<p>&quot;Default&quot; namespace, Pod labeled with any &quot;role = frontend&quot;; and<br>
namespaces in the pod with any label &quot;project = myproject&quot;; and<br>
172.17.0.0-172.17.0.255 range of IP addresses and<br>
172.17.2.0-172.17.255.255 (i.e., all except 172.17.1.0/24 172.17.0.0/16) of<br>
Allow with the &quot;project = myproject&quot; pod at any namespace tag 6379 is connected to the TCP port of the &quot;default&quot; under the name space pod.</p>
<h3 id="selector-to-and-from-the-behavior">Selector to and from the behavior</h3>
<p>Four kinds of selectors can be specified in the ingress from section to section or egress:</p>
<p>podSelector: This selects a specific Pod NetworkPolicy with the same name space, the inlet should be permitted as a source or destination outlet.</p>
<p>namespaceSelector: This selects a particular namespace, all Pod should be used as an input source or output destination.</p>
<p>namespaceSelector and podSelector: a designated to namespaceSelector and the podSelector / from a particular entry selection namespace specific Pod. Note the use of the correct YAML syntax; this strategy:</p>
<p>...<br>
ingress:</p>
<ul>
<li>from:
<ul>
<li>namespaceSelector:<br>
matchLabels:<br>
user: alice<br>
podSelector:<br>
matchLabels:<br>
role: client<br>
...<br>
From the array contains only one element, only marked role = client from the name space of the Pod Pod and labeled where user = alice connection. This strategy:</li>
</ul>
</li>
</ul>
<p>...<br>
ingress:</p>
<ul>
<li>from:
<ul>
<li>namespaceSelector:<br>
matchLabels:<br>
user: alice</li>
<li>podSelector:<br>
matchLabels:<br>
role: client<br>
...<br>
Contained in two elements from the array, from the local namespace allows marked role = Pod connection of the client, or from any namespace labeled user = alice any connection of the Pod.</li>
</ul>
</li>
</ul>
<p>ipBlock: This selects a particular IP CIDR range as an inlet or source outlet destinations. These should be outside the cluster IP, because time is short Pod IP presence and randomly generated.</p>
<p>Inlet and outlet mechanisms usually need to rewrite the cluster IP source or destination IP packet. In the case of this happening, uncertainty occurs before or after NetworkPolicy processing and plug-ins for different combinations of networks, cloud providers, Service realization, the behavior may be different.</p>
<p>In the case of entry, which means that in some cases, you can filter incoming packets based on the actual source of the original IP, while in other cases, NetworkPolicy the role of the IP source or it may be LoadBalancer Pod node Wait.</p>
<p>For exports, which means that from the Pod to be rewritten as a cluster of Service IP connectivity to external IP may or may not be bound by the policy-based ipBlock.</p>
<h3 id="the-default-policy">The default policy</h3>
<p>By default, if there is no policy namespace, all traffic in and out of the namespace Pod will be allowed. The following example lets you change the default behavior of the name space.</p>
<h3 id="default-deny-all-traffic-entry">Default deny all traffic entry</h3>
<p>You can select all containers but not by creating any NetworkPolicy inlet flow into the container to create these &quot;default&quot; isolation strategy for the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
</code></pre>
<p>This ensures that even if the container NetworkPolicy no other choice, it can still be isolated. This policy will not change the default behavior export isolation.</p>
<p>The default allows all ingress traffic<br>
If you want to allow all traffic entering all Pod a namespace (even if you add the policy causes some Pod is considered &quot;isolated&quot;), you can create a policy to explicitly permit all traffic to the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
</code></pre>
<p>Default Deny all export traffic<br>
You can select all container exports but not by creating any traffic NetworkPolicy from these containers to create a &quot;default&quot; egress isolation strategy for the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Egress
</code></pre>
<p>This ensures that even if not any other choice NetworkPolicy Pod will not be allowed out of traffic. This policy will not change the default ingress isolation behavior.</p>
<p>The default allows all export traffic<br>
If you want to allow all traffic from the namespace of all Pod (even if you add a policy causes some Pod is considered &quot;isolated&quot; in), you can create a policy that explicitly permits all traffic exit the namespace.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress
</code></pre>
<p>Default Deny all entry and exit of all traffic<br>
You can create a &quot;default&quot; policy for the namespace, by creating the following NetworkPolicy in the namespace block all inbound and outbound traffic.</p>
<pre><code class="language-yaml">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
</code></pre>
<p>This ensures that even if not any other choice NetworkPolicy Pod will not be allowed into or out of traffic.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Golang Learning Blog List]]></title>
        <id>https://jiaming-hu.com/post/golang-learning-blog/</id>
        <link href="https://jiaming-hu.com/post/golang-learning-blog/">
        </link>
        <updated>2020-04-15T02:49:45.000Z</updated>
        <content type="html"><![CDATA[<h2 id="basic">Basic</h2>
<p><a href="http://devs.cloudimmunity.com/gotchas-and-common-mistakes-in-go-golang/">50 Shades of Go: Traps, Gotchas, and Common Mistakes for New Golang Devs</a></p>
<h2 id="kubernetes">Kubernetes</h2>
<p><a href="https://www.openshift.com/blog/kubernetes-deep-dive-code-generation-customresources">Code Generation for CustomResources</a></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 8 | Secret type, creation, Pod use, and ServiceAccount associated Secret]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-8/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-8/">
        </link>
        <updated>2020-04-13T23:52:18.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create a secret named cka1127-secret, which contains a password field with a value of cka1127, and then use ENV to call in Pod1 named cka1127-01, and use Volume to mount under /data in Pod2 named cka1127-02;</p>
<h2 id="answer">Answer</h2>
<p>###Create secret</p>
<h4 id="use-yaml-file">use YAML file</h4>
<p>cka1127-secret.yaml:</p>
<pre><code>apiVersion: v1
kind: Secret
metadata:
  name: cka1127-secret
type: Opaque
stringData:
  password: cka1127
</code></pre>
<pre><code>kubectl apply -f  cka-1127-secret.yaml 
</code></pre>
<h4 id="using-cli">using CLI</h4>
<pre><code>kubectl create secret generic cka1127-secret --from-literal=password=cka1127
</code></pre>
<h3 id="create-pod-cka1127-01">Create pod cka1127-01</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka1127-01
spec:
  containers:
  - image: nginx
    name: nginx
    env:
    - name: PASSWORD
       valueFrom:
        secretKeyRef:
          name: cka1127-secret
          key: password
</code></pre>
<pre><code>kubectl exec -ti cka1127-01  bash
root@cka1127-01:/# 
root@cka1127-01:/# echo $PASSWORD
cka1127
root@cka1127-01:/#
</code></pre>
<h3 id="create-pod-cka1127-02">Create pod cka1127-02</h3>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka1127-02
spec:
  containers:
  - name: nginx
    image: nginx
    volumeMounts:
    - name: pwd
       mountPath: /data
       readOnly: true
  volumes:
  - name: pwd
     secret:
       secretName: cka1127-secret
</code></pre>
<pre><code># kubectl exec -ti cka1127-02 sh
# 
# ls -l /data
total 0
lrwxrwxrwx 1 root root 15 Nov 28 00:40 password -&gt; ..data/password
# cat /data/password
cka1127# 
</code></pre>
<h2 id="introduction">Introduction</h2>
<p>K8s offical document of secret https://kubernetes.io/docs/concepts/configuration/secret/</p>
<h3 id="introduction-to-secret">Introduction to secret</h3>
<p><code>Secret</code> resources in Kubernetes can be used to store sensitive data such as passwords, tokens, and secret keys, and store these sensitive information in <code>Secret</code>, which is more secure and flexible than exposure to Pods and images.</p>
<p>You may think that secret is generally not used. In fact, when creating a Pod, Kubernetes automatically creates a secret containing the credentials used to access the API (controlled by the <code>service account token controller</code> of kube-controller-manager), and it will Automatically modify the Pod to use this type of secret (this is controlled by the <code>Admission Controller</code>).</p>
<p>Each namespace of k8s will have a Service Account. When we create a namespace, the service account controller will monitor the creation of the namespace, a service account named default will be created in the namespace, and the service account token controller will monitor the creation of the service account , Create the corresponding secret, and bind this secret to the Service Account.</p>
<pre><code># kubectl create ns cka
namespace/cka created
# kubectl get sa -n cka
NAME      SECRETS   AGE
default   1         11s
# kubectl get secrets -n cka
NAME                  TYPE                                  DATA   AGE
default-token-r77xn   kubernetes.io/service-account-token   3      18s
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586911624680.png" alt="" loading="lazy"></figure>
<p>When we create a Pod, if no Service Account is specified, it will automatically be assigned a Service Account in the same command space namespace by default. You can see that the spec.serviceAccountName field has been automatically set.</p>
<figure data-type="image" tabindex="2"><img src="https://jiaming-hu.com/post-images/1586911645342.jpeg" alt="" loading="lazy"></figure>
<p>You can use the automatically added Service Account credentials to access the API from within the Pod. The API permissions of the Service Account depend on the authorization plugin and policy used.</p>
<p>For more information about how Service Account works, please see the documentation: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/</p>
<h3 id="types-of-secret">Types of Secret</h3>
<p><code>--type</code> specifies the type of secret created, Kubernetes has built-in three types of Secret</p>
<h4 id="kubernetesio-service-account-token-secret">kubernetes.io / service-account-token Secret</h4>
<p>As mentioned above, in order to access the Kubernetes API from within the Pod, Kubernetes provides Service Account resources. The Service Account will automatically create and mount the Secret that accesses the Kubernetes API, and will be mounted in the Pod's /var/run/secrets/kubernetes.io/serviceaccount directory. The Secret used by the Service Account that is automatically created when the namespace is created is of this type.<br>
<img src="https://jiaming-hu.com/post-images/1586911898128.png" alt="" loading="lazy"></p>
<h4 id="opaque-secret">Opaque Secret</h4>
<p>The Opaque type Secret is a map structure (key-value), where vlaue requires encoding in base64 format. In the following examples, it is basically the Opaque type Secret.</p>
<h4 id="kubernetesio-dockerconfigjson-secret">kubernetes.io / dockerconfigjson Secret</h4>
<p>Secret of type <code>kubernetes.io/dockercfg</code> is used to store private Docker Registry authentication information. When Kubernetes creates a Pod and needs to pull the image from the private Docker Registry, it needs to use the authentication information, and it will use the kubernetes.io/dockercfg type Secret.</p>
<pre><code>kubectl create secret docker-registry cka-regsecret \
--docker-server=coderaction \
--docker-username=admin \
--docker-password=123456 \
--docker-email=837448191@qq.com
</code></pre>
<p>When creating a Pod, you need to use it in the Pod spec as follows:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka-private-reg
spec:
  containers:
    - name: cka-private-reg-container
      image: nginx
  imagePullSecrets:
    - name: cka-regsecret
</code></pre>
<p>You can refer to the official documentation: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod</p>
<p>Add imagePullSecrets to the default ServiceAccount of the partition, so that all Pods created can automatically add spec.imagePullSecrets. For details, refer to the official document: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ # add-imagepullsecrets-to-a-service-account</p>
<h3 id="create-secret">Create secret</h3>
<h4 id="use-kubectl-to-create-secret">Use kubectl to create secret</h4>
<p>kubectl creates seccret official document: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-secret-em-</p>
<p>format</p>
<pre><code>kubectl create secret generic NAME [--type=string] [--from-file=[key=]source] [--from-literal=key1=value1] [--dry-run]
</code></pre>
<p>--from-literal: Specify the key and text value to be inserted (that is, mykey = somevalue), the value is a plain text value, and it will be encoded by base64 after creation; Files, in this case, will be assigned a default name; or you can choose to use the specified directory, which will iterate over each valid file key in that directory. --type: The type of secret created. Three types have been introduced above; --dry-run: If true, only the created object will be sent to APIServer without sending it. The default value is false</p>
<h3 id="create-secret-by-from-file">create secret by <code>--from-file</code></h3>
<pre><code># echo -n 'admin' &gt; ./username
# echo -n 'test123' &gt; ./password
# echo -n 'shanghai' &gt; ./city
# ll
total 12
-rw-r--r-- 1 root root 9 Nov 28 20:44 city
-rw-r--r-- 1 root root 8 Nov 28 20:43 password
-rw-r--r-- 1 root root 6 Nov 28 20:43 username
# kubectl create secret generic test-cka1127-01 --from-file=./username --from-file=./password
secret/test-cka1127-01 created
# kubectl create secret generic test-cka1127-02 --from-file=./
secret/test-cka1127-02 created
# 
</code></pre>
<p>With the secret created in the specified directory, all files in the directory are added to the data:</p>
<pre><code>kubectl get secrets test-cka1127-02 -oyaml
apiVersion: v1
data:
  city: c2hhbmdoYWkK
  password: dGVzdDEyMwo=
  username: YWRtaW4K
kind: Secret
metadata:
  creationTimestamp: &quot;2019-11-28T12:44:57Z&quot;
  name: test-cka1127-02
  namespace: default
  resourceVersion: &quot;14636360&quot;
  selfLink: /api/v1/namespaces/default/secrets/test-cka1127-02
  uid: 4a3a1a5d-09e6-4bf9-bbe3-3300db1ddf7a
type: Opaque
</code></pre>
<p>Specify the secret created by username and password:</p>
<pre><code>kubectl get secrets test-cka1127-01 -oyaml
apiVersion: v1
data:
  password: dGVzdDEyMwo=
  username: YWRtaW4K
kind: Secret
metadata:
  creationTimestamp: &quot;2019-11-28T12:44:47Z&quot;
  name: test-cka1127-01
  namespace: default
  resourceVersion: &quot;14636347&quot;
  selfLink: /api/v1/namespaces/default/secrets/test-cka1127-01
  uid: 766516a2-34be-4a18-b4e2-83751a6cd2b7
type: Opaque
</code></pre>
<h3 id="create-secret-from-the-generator">Create Secret from the generator</h3>
<p>Starting from 1.14, Kubectl supports Kustomize to manage objects. With this new feature, you can also create a Secret from the generator and then apply it to create objects on Apiserver.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 7 | InitContainer concept, usage, usage scenario introduction; k8s secret env, volume exam questions]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-7/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-7/">
        </link>
        <updated>2020-04-11T00:49:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Provide a yaml of the pod, request to add Init Container, the role of Init Container is to create an empty file, pod container to determine whether the file exists, otherwise, exit.</p>
<h2 id="answer">Answer</h2>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    run: cka-1126
  name: cka-1126
spec:
  initContainers:
  - image: busybox
    name: init-c
    command: ['sh', '-c', 'touch /tmp/cka-1126']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  containers:
  - image: busybox
    name: cka-1126
    command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  volumes:
  - name: workdir
    emptyDir: {}
</code></pre>
<p>The command of the Container is to determine whether the file exists, not exit if it exists, and exit if it does not exist; you can also use the following if to determine:</p>
<pre><code>command: ['sh', '-c', 'if [ -e /tmp/cka-1126 ];then echo &quot;file exits&quot;;else echo &quot;file not exits&quot; &amp;&amp; exit 1;fi']
</code></pre>
<h3 id="init-container">init container</h3>
<p>The key point of this question is that the init container and the main container need to mount a directory called workdir together. The init container creates an empty file in it. The main container checks whether the file exists, and the main syntax is shell syntax;</p>
<pre><code>command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
</code></pre>
<p>This sentence means: If the return code of <code>ls /tmp/cka-1126</code> is 0, that is the file exists, sleep 3600 seconds; otherwise exit 1 exits;</p>
<p>You can also use the <code>if</code> syntax to judge.</p>
<p>Official document address:<br>
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/<br>
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</p>
<h3 id="carding-concepts">Carding concepts</h3>
<p>Initialize the container. As the name suggests, when the container is started, it will start one or more containers. If there are multiple containers, then these Init Containers are executed in order according to the defined order. If one is executed successfully, the next one can be executed. Only all Init After the Container is executed, the main container will start. Since the storage volume in a Pod is shared, the data generated in the Init Container can be used by the main container.</p>
<p>Init Container can be used in a variety of K8S resources such as Deployment, Daemon Set, StatefulSet, Job, etc. But in the final analysis, it is executed when the Pod starts, before the main container starts, and does the initialization work.</p>
<p>The Init container supports all fields and features of the application container, including resource limits, data volumes, and security settings. However, Init containers do not support Readiness Probes because they must be completed before the Pod is ready; there will be slight differences in resource limits and scheduling.</p>
<p>###Application scenario<br>
Wait for other modules Ready: For example, there is an application with two containerized services, one is Web Server and the other is database. The Web Server needs to access the database. However, when we start this application, there is no guarantee that the database service will start first, so there may be errors in the Web Server connecting to the database for a period of time. In order to solve this problem, we can use an InitContainer in the Pod running the Web Server service to check whether the database is ready, until the database can be connected, the Init Container ends and exits, and then the Web Server container is started to initiate a formal database connection request .</p>
<p>Initial configuration: for example, to detect all existing member nodes in the cluster and prepare the configuration information of the cluster for the main container, so that the main container can use this configuration information to join the cluster when it is up; currently in containerization, it is often used when initializing the cluster configuration file Arrive</p>
<p>Provide a way to block the start of the container: the next container must be run after the initContainer container is successfully started, which guarantees a successful way of running a set of conditions;</p>
<p>Other usage scenarios: register pods to a central database, download application dependencies, etc.</p>
<p>Kubernetes version 1.5 began to support pod.beta.kubernetes.io/init-containers to declare initContainer under annotations, like the following.</p>
<pre><code>
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
  annotations:
    pod.beta.kubernetes.io/init-containers: '[
        {
            &quot;name&quot;: &quot;init-myservice&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&quot;]
        },
        {
            &quot;name&quot;: &quot;init-mydb&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&quot;]
        }
    ]'
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
</code></pre>
<p>The new syntax of Kubernetes version 1.6 moved the declaration of the Init container to spec, but the old annotation syntax can still be used.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 6 | Deployment upgrade, rollback, rolling update strategy, roll, set image command explanation, initContainer exam questions]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-6/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-6/">
        </link>
        <updated>2020-04-10T13:32:51.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Through the command line, create a deployment, the number of copies is 3, and the image is nginx: latest. Then scroll to nginx: 1.9.1, and then roll back to the original version</p>
<p>Requirements: The name of the deployment is cka-1125, and the related commands used are posted.</p>
<p>It is best to attach the completed deployment yaml and the commands related to the upgrade rollback.</p>
<h2 id="anwser">Anwser</h2>
<p>Create a deployment first, you can use the command to create:</p>
<pre><code>kubectl run cka-1125  --image=nginx --replicas=3
</code></pre>
<p>You can also use the following yaml: cka-1125.yaml to create</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1125
  name: cka-1125
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cka-1125
  template:
    metadata:
      labels:
        app: cka-1125
    spec:
      containers:
      - image: nginx
        name: cka-1125
</code></pre>
<p>Create:</p>
<pre><code>kubectl apply -f cka-1125.yaml
</code></pre>
<p>Upgrade:</p>
<pre><code>kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
</code></pre>
<p>Rollback:</p>
<pre><code># Rollback to the previous version
kubectl rollout undo deploy/cka-1125
# Rollback to the specified version
kubectl rollout undo deploy/cka-1125 --to-revision=2
</code></pre>
<p>the offical document of the set image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set</p>
<h3 id="set-image">Set image</h3>
<p>Set image command:<br>
The format of the set image command is as follows:</p>
<pre><code>kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [--record]
</code></pre>
<p><code>--record</code> specifies that the current kubectl command is recorded in the annotation. If set to <code>false</code>, the command is not recorded. If set to <code>true</code>, the command is recorded. The default is <code>false</code>.</p>
<pre><code>[root@liabio test]# kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
[root@liabio test]# 
[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
3         &lt;none&gt;
4         kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record=true
</code></pre>
<p>As above, there will be an upgrade command in CHANGE-CAUSE.<br>
The set image command can operate on: <code>pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), replicaset (rs), statefulset (sts)</code>.</p>
<h3 id="roll-command">Roll command</h3>
<p>the offical document of the roll image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout<br>
The roll command can manage the rollback of deployments, daemonsets, statefulsets resources:</p>
<p>Query upgrade history:</p>
<pre><code>[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
</code></pre>
<p>Check the details of the specified version:</p>
<pre><code>kubectl rollout history deploy/cka-1125 --revision=3 -o=yaml
</code></pre>
<p>Rollback to the previous version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Rollback to the specified version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 --to-revision=3
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Other roll subcommands:</p>
<ul>
<li>
<p>restart: the resource will restart; status: show the rollback status;</p>
</li>
<li>
<p>resume: resume the suspended resource. The controller does not control the suspended resources. By restoring resources, the controller can be controlled again. resume is only supported for deployment.</p>
</li>
<li>
<p>pause: The controller will not control the paused resource. Use kubectl rollout resume to resume suspended resources. Currently, only deployment support is suspended.</p>
</li>
</ul>
<h3 id="rolling-update-strategy">Rolling update strategy</h3>
<p>the offical document of rollingupdate<br>
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p>
<pre><code>minReadySeconds: 5
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
</code></pre>
<p><code>minReadySeconds</code> Kubernetes upgrades after waiting for the set time. If this value is not set, Kubernetes will assume that the container will provide services after it is started. If this value is not set, it may cause the service service to run normally in some extreme cases.</p>
<p><code>maxSurge</code> controls the total number of copies in the rolling update process to exceed the upper limit of DESIRED. maxSurge can be a specific integer or a percentage, rounded up. The default value of maxSurge is 25%.</p>
<p>For example, DESIRED is 10, then the maximum number of copies is roundUp (10 + 10 * 25%) = 13, so CURRENT is 13.</p>
<p><code>maxUnavaible</code> controls the maximum percentage of unavailable copies in DESIRED during the rolling update process. maxUnavailable can be a specific integer or 100%, rounded down. The default value is 25%.</p>
<p>For example, DESIRED is 10, then the number of available copies must be at least 10-roundDown (10 * 25%) = 8, so AVAILABLE is 8.</p>
<p><code>The larger maxSurge, the more new copies created initially; the larger maxUnavailable, the more old copies originally destroyed.</code></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day5 | Advanced scheduling of kube-scheduler scheduler: affinity anti-affinity scheduling, deployment upgrade rollback]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day5/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day5/">
        </link>
        <updated>2020-04-09T12:27:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>By using the command line, create two deployments.</p>
<p>There are 2 nodes in the cluster:</p>
<p>The first deployment name is cka-1122-01, using nginx image, there are 2 pods, and configure the deployment itself to anti-affinity at the node level between pods;</p>
<p>The second deployment name is cka-1122-02, using the nginx image, there are 2 pods, and configure the deployment pod have affinity with the first deployment pod at the node level;</p>
<h2 id="answer">Answer</h2>
<ul>
<li>The first deployment: cka-1122-01</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-01
  name: cka-1122-01
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-01
  template:
    metadata:
      labels:
        app: cka-1122-01
    spec:
      containers:
      - image: nginx
        name: cka-1122-01  
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cka-1122-01
              topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<ul>
<li>The second deployment: cka-1122-02</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-02
  name: cka-1122-02
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-02
  template:
    metadata:
      labels:
        app: cka-1122-02
    spec:
      containers:
      - image: nginx
        name: cka-1122-02
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cka-1122-01
            topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<p>The schedule result:</p>
<pre><code>NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
cka-1122-01-5df9bdf8c9-qwd2v    1/1      Running      0       8m     10.192.4.2     node-1
cka-1122-01-5df9bdf8c9-r4rhs    1/1      Running      0       8m     10.192.4.3     node-2  
cka-1122-02-749cd4b846-bjhzq    1/1      Running      0       10m    10.192.4.4     node-1
cka-1122-02-749cd4b846-rkgpo    1/1      Running      0       10m    10.192.4.5     node-2  
</code></pre>
<p>Affinity and Anti-Affinity document:<br>
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</p>
]]></content>
    </entry>
</feed>