<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://jiaming-hu.com</id>
    <title>Jiaming&apos;s Blog</title>
    <updated>2020-04-13T01:10:19.807Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://jiaming-hu.com"/>
    <link rel="self" href="https://jiaming-hu.com/atom.xml"/>
    <subtitle>Why’s THE Design</subtitle>
    <logo>https://jiaming-hu.com/images/avatar.png</logo>
    <icon>https://jiaming-hu.com/favicon.ico</icon>
    <rights>All rights reserved 2020, Jiaming&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions --Day 7]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-7/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-7/">
        </link>
        <updated>2020-04-12T12:49:47.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Provide a yaml of the pod, request to add Init Container, the role of Init Container is to create an empty file, pod container to determine whether the file exists, otherwise, exit.</p>
<h2 id="answer">Answer</h2>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  labels:
    run: cka-1126
  name: cka-1126
spec:
  initContainers:
  - image: busybox
    name: init-c
    command: ['sh', '-c', 'touch /tmp/cka-1126']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  containers:
  - image: busybox
    name: cka-1126
    command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
    volumeMounts:
    - name: workdir
      mountPath: &quot;/tmp&quot;
  volumes:
  - name: workdir
    emptyDir: {}
</code></pre>
<p>The command of the Container is to determine whether the file exists, not exit if it exists, and exit if it does not exist; you can also use the following if to determine:</p>
<pre><code>command: ['sh', '-c', 'if [ -e /tmp/cka-1126 ];then echo &quot;file exits&quot;;else echo &quot;file not exits&quot; &amp;&amp; exit 1;fi']
</code></pre>
<h3 id="init-container">init container</h3>
<p>The key point of this question is that the init container and the main container need to mount a directory called workdir together. The init container creates an empty file in it. The main container checks whether the file exists, and the main syntax is shell syntax;</p>
<pre><code>command: ['sh', '-c', 'ls /tmp/cka-1126 &amp;&amp; sleep 3600 || exit 1']
</code></pre>
<p>This sentence means: If the return code of <code>ls /tmp/cka-1126</code> is 0, that is the file exists, sleep 3600 seconds; otherwise exit 1 exits;</p>
<p>You can also use the <code>if</code> syntax to judge.</p>
<p>Official document address:<br>
https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/<br>
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</p>
<h3 id="carding-concepts">Carding concepts</h3>
<p>Initialize the container. As the name suggests, when the container is started, it will start one or more containers. If there are multiple containers, then these Init Containers are executed in order according to the defined order. If one is executed successfully, the next one can be executed. Only all Init After the Container is executed, the main container will start. Since the storage volume in a Pod is shared, the data generated in the Init Container can be used by the main container.</p>
<p>Init Container can be used in a variety of K8S resources such as Deployment, Daemon Set, StatefulSet, Job, etc. But in the final analysis, it is executed when the Pod starts, before the main container starts, and does the initialization work.</p>
<p>The Init container supports all fields and features of the application container, including resource limits, data volumes, and security settings. However, Init containers do not support Readiness Probes because they must be completed before the Pod is ready; there will be slight differences in resource limits and scheduling.</p>
<p>###Application scenario<br>
Wait for other modules Ready: For example, there is an application with two containerized services, one is Web Server and the other is database. The Web Server needs to access the database. However, when we start this application, there is no guarantee that the database service will start first, so there may be errors in the Web Server connecting to the database for a period of time. In order to solve this problem, we can use an InitContainer in the Pod running the Web Server service to check whether the database is ready, until the database can be connected, the Init Container ends and exits, and then the Web Server container is started to initiate a formal database connection request .</p>
<p>Initial configuration: for example, to detect all existing member nodes in the cluster and prepare the configuration information of the cluster for the main container, so that the main container can use this configuration information to join the cluster when it is up; currently in containerization, it is often used when initializing the cluster configuration file Arrive</p>
<p>Provide a way to block the start of the container: the next container must be run after the initContainer container is successfully started, which guarantees a successful way of running a set of conditions;</p>
<p>Other usage scenarios: register pods to a central database, download application dependencies, etc.</p>
<p>Kubernetes version 1.5 began to support pod.beta.kubernetes.io/init-containers to declare initContainer under annotations, like the following.</p>
<pre><code>
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
  annotations:
    pod.beta.kubernetes.io/init-containers: '[
        {
            &quot;name&quot;: &quot;init-myservice&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup myservice; do echo waiting for myservice; sleep 2; done;&quot;]
        },
        {
            &quot;name&quot;: &quot;init-mydb&quot;,
            &quot;image&quot;: &quot;busybox&quot;,
            &quot;command&quot;: [&quot;sh&quot;, &quot;-c&quot;, &quot;until nslookup mydb; do echo waiting for mydb; sleep 2; done;&quot;]
        }
    ]'
spec:
  containers:
  - name: myapp-container
    image: busybox
    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']
</code></pre>
<p>The new syntax of Kubernetes version 1.6 moved the declaration of the Init container to spec, but the old annotation syntax can still be used.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 6]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-6/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-6/">
        </link>
        <updated>2020-04-11T13:32:51.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Through the command line, create a deployment, the number of copies is 3, and the image is nginx: latest. Then scroll to nginx: 1.9.1, and then roll back to the original version</p>
<p>Requirements: The name of the deployment is cka-1125, and the related commands used are posted.</p>
<p>It is best to attach the completed deployment yaml and the commands related to the upgrade rollback.</p>
<h2 id="anwser">Anwser</h2>
<p>Create a deployment first, you can use the command to create:</p>
<pre><code>kubectl run cka-1125  --image=nginx --replicas=3
</code></pre>
<p>You can also use the following yaml: cka-1125.yaml to create</p>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1125
  name: cka-1125
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cka-1125
  template:
    metadata:
      labels:
        app: cka-1125
    spec:
      containers:
      - image: nginx
        name: cka-1125
</code></pre>
<p>Create:</p>
<pre><code>kubectl apply -f cka-1125.yaml
</code></pre>
<p>Upgrade:</p>
<pre><code>kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
</code></pre>
<p>Rollback:</p>
<pre><code># Rollback to the previous version
kubectl rollout undo deploy/cka-1125
# Rollback to the specified version
kubectl rollout undo deploy/cka-1125 --to-revision=2
</code></pre>
<p>the offical document of the set image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#set</p>
<h3 id="set-image">Set image</h3>
<p>Set image command:<br>
The format of the set image command is as follows:</p>
<pre><code>kubectl set image (-f FILENAME | TYPE NAME) CONTAINER_NAME_1=CONTAINER_IMAGE_1 ... CONTAINER_NAME_N=CONTAINER_IMAGE_N [--record]
</code></pre>
<p><code>--record</code> specifies that the current kubectl command is recorded in the annotation. If set to <code>false</code>, the command is not recorded. If set to <code>true</code>, the command is recorded. The default is <code>false</code>.</p>
<pre><code>[root@liabio test]# kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record
deployment.extensions/cka-1125 image updated
[root@liabio test]# 
[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
3         &lt;none&gt;
4         kubectl set image deploy/cka-1125 cka-1125=nginx:1.9.1 --record=true
</code></pre>
<p>As above, there will be an upgrade command in CHANGE-CAUSE.<br>
The set image command can operate on: <code>pod (po), replicationcontroller (rc), deployment (deploy), daemonset (ds), replicaset (rs), statefulset (sts)</code>.</p>
<h3 id="roll-command">Roll command</h3>
<p>the offical document of the roll image commnad<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#rollout<br>
The roll command can manage the rollback of deployments, daemonsets, statefulsets resources:</p>
<p>Query upgrade history:</p>
<pre><code>[root@liabio test]# kubectl rollout history deploy/cka-1125 
deployment.extensions/cka-1125 
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
</code></pre>
<p>Check the details of the specified version:</p>
<pre><code>kubectl rollout history deploy/cka-1125 --revision=3 -o=yaml
</code></pre>
<p>Rollback to the previous version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Rollback to the specified version</p>
<pre><code>[root@liabio test]# kubectl rollout undo deploy/cka-1125 --to-revision=3
deployment.extensions/cka-1125 rolled back
</code></pre>
<p>Other roll subcommands:</p>
<ul>
<li>
<p>restart: the resource will restart; status: show the rollback status;</p>
</li>
<li>
<p>resume: resume the suspended resource. The controller does not control the suspended resources. By restoring resources, the controller can be controlled again. resume is only supported for deployment.</p>
</li>
<li>
<p>pause: The controller will not control the paused resource. Use kubectl rollout resume to resume suspended resources. Currently, only deployment support is suspended.</p>
</li>
</ul>
<h3 id="rolling-update-strategy">Rolling update strategy</h3>
<p>the offical document of rollingupdate<br>
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</p>
<pre><code>minReadySeconds: 5
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxSurge: 1
    maxUnavailable: 1
</code></pre>
<p><code>minReadySeconds</code> Kubernetes upgrades after waiting for the set time. If this value is not set, Kubernetes will assume that the container will provide services after it is started. If this value is not set, it may cause the service service to run normally in some extreme cases.</p>
<p><code>maxSurge</code> controls the total number of copies in the rolling update process to exceed the upper limit of DESIRED. maxSurge can be a specific integer or a percentage, rounded up. The default value of maxSurge is 25%.</p>
<p>For example, DESIRED is 10, then the maximum number of copies is roundUp (10 + 10 * 25%) = 13, so CURRENT is 13.</p>
<p><code>maxUnavaible</code> controls the maximum percentage of unavailable copies in DESIRED during the rolling update process. maxUnavailable can be a specific integer or 100%, rounded down. The default value is 25%.</p>
<p>For example, DESIRED is 10, then the number of available copies must be at least 10-roundDown (10 * 25%) = 8, so AVAILABLE is 8.</p>
<p><code>The larger maxSurge, the more new copies created initially; the larger maxUnavailable, the more old copies originally destroyed.</code></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day5]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day5/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day5/">
        </link>
        <updated>2020-04-10T12:27:36.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>By using the command line, create two deployments.</p>
<p>There are 2 nodes in the cluster:</p>
<p>The first deployment name is cka-1122-01, using nginx image, there are 2 pods, and configure the deployment itself to anti-affinity at the node level between pods;</p>
<p>The second deployment name is cka-1122-02, using the nginx image, there are 2 pods, and configure the deployment pod have affinity with the first deployment pod at the node level;</p>
<h2 id="answer">Answer</h2>
<ul>
<li>The first deployment: cka-1122-01</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-01
  name: cka-1122-01
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-01
  template:
    metadata:
      labels:
        app: cka-1122-01
    spec:
      containers:
      - image: nginx
        name: cka-1122-01  
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - cka-1122-01
              topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<ul>
<li>The second deployment: cka-1122-02</li>
</ul>
<pre><code>apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: cka-1122-02
  name: cka-1122-02
spec:
  replicas: 2
  selector:
    matchLabels:
      app: cka-1122-02
  template:
    metadata:
      labels:
        app: cka-1122-02
    spec:
      containers:
      - image: nginx
        name: cka-1122-02
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - cka-1122-01
            topologyKey: &quot;kubernetes.io/hostname&quot;
</code></pre>
<p>The schedule result:</p>
<pre><code>NAME                           READY     STATUS    RESTARTS   AGE       IP           NODE
cka-1122-01-5df9bdf8c9-qwd2v    1/1      Running      0       8m     10.192.4.2     node-1
cka-1122-01-5df9bdf8c9-r4rhs    1/1      Running      0       8m     10.192.4.3     node-2  
cka-1122-02-749cd4b846-bjhzq    1/1      Running      0       10m    10.192.4.4     node-1
cka-1122-02-749cd4b846-rkgpo    1/1      Running      0       10m    10.192.4.5     node-2  
</code></pre>
<p>Affinity and Anti-Affinity document:<br>
https://kubernetes.io/docs/concepts/configuration/assign-pod-node/</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 4]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day4/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day4/">
        </link>
        <updated>2020-04-08T18:19:39.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Through the command line, use the nginx image to create a pod and manually schedule it to a node named node1121. The name of the pod is cka-1121. The best answer is attached. The command used to create the pod is the most streamlined yaml.<br>
if the comment is there Restrictions, please list the points of attention, mainly need to list how to do manual scheduling?</p>
<p>Note: Manual scheduling means that there is no need to go through kube-scheduler to schedule.</p>
<h2 id="answer">Answer</h2>
<p>Schedule the Pod named cka-1121 to node node 1121:</p>
<pre><code>apiVersion: v1
kind: Pod
metadata:
  name: cka-1121
  labels:
    app: cka-1121
spec:
  containers:
  - name: cka-1121
    image: busybox
    command: ['sh', '-c', 'echo Hello CKA! &amp;&amp; sleep 3600']
  nodeName: node1121
</code></pre>
<p>Address of the scheduler in the official website:</p>
<p>https://kubernetes.io/docs/concepts/scheduling/kube-scheduler/</p>
<p>Scheduler command line parameters:</p>
<p>https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/</p>
<p>The scheduler <code>kube-scheduler</code> is divided into <code>pre-selection</code>, <code>optimization</code>, <code>pod priority preemption</code>, and <code>bind</code> phase;</p>
<p><code>Pre-selection</code>: Pop the pods that need to be scheduled from the queue of podQueue to be scheduled. Enter the pre-selection stage first. The pre-selection function determines whether each node is suitable for scheduling by the pod.</p>
<p><code>Preferably</code>: select the optimal node from the satisfied nodes selected by the pre-selection.</p>
<p><code>Pod priority preemption</code>: If the pre-selection and preferred scheduling fail, it will try to remove the pods with low priority and make the pods with high priority scheduled successfully.</p>
<p><code>bind</code>: After the above steps are completed, the scheduler will update the local cache, but in the end, you need to submit the binding result to etcd, you need to call the Bind interface of Apiserver to complete.</p>
<blockquote>
<p>The following k8s source code version is 1.18.1<br>
Let's check the kube-scheduler source code. The scheduler uses the list-watch mechanism to monitor the Pod's new, updated, and deleted events in the cluster, and calls the callback function. After the nodeName is specified, it will not be put into the unscheduled podQueue queue, and it will not go through the above stages.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/scheduler.go#L234<br>
Among them, when constructing callback functions for adding, updating, and deleting pod resource objects, divided into scheduled and unscheduled callbacks.</p>
</blockquote>
<p>Scheduled pod callback:</p>
<p>The scheduled pods are filtered according to the logic defined in FilterFunc. When nodeName is not empty, when it returns true, it will go to AddFunc, UpdateFunc, and DeleteFunc defined in Handler. In the cache, because the scheduler maintains a cache of pod lists on the node.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L356</p>
<pre><code>	// scheduled pod cache
	podInformer.Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return assignedPod(t)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return assignedPod(pod)
					}
					utilruntime.HandleError(fmt.Errorf(&quot;unable to convert object %T to *v1.Pod in %T&quot;, obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf(&quot;unable to handle object in %T: %T&quot;, sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)
</code></pre>
<p>Unscheduled pod callbacks:</p>
<p>Unscheduled pods are filtered according to the logic defined in FilterFunc. When nodeName is empty and pod ’s SchedulerName matches the scheduler ’s name, true is returned; when true is returned, AddFunc, UpdateFunc, and DeleteFunc defined in Handler will be used. Will be added to podQueue.<br>
https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/eventhandlers.go#L380</p>
<pre><code>	// unscheduled pod queue
	podInformer.Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return !assignedPod(t) &amp;&amp; responsibleForPod(t, sched.Profiles)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return !assignedPod(pod) &amp;&amp; responsibleForPod(pod, sched.Profiles)
					}
					utilruntime.HandleError(fmt.Errorf(&quot;unable to convert object %T to *v1.Pod in %T&quot;, obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf(&quot;unable to handle object in %T: %T&quot;, sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToSchedulingQueue,
				UpdateFunc: sched.updatePodInSchedulingQueue,
				DeleteFunc: sched.deletePodFromSchedulingQueue,
			},
		},
	)
</code></pre>
<ul>
<li>
<p>Applicable scenarios for manual scheduling:</p>
<p>When the scheduler is not working, you can set nodeName for temporary emergency;<br>
Can be packaged into its own scheduler;</p>
</li>
<li>
<p>Extension point:</p>
<p>In the past several versions of Daemonset, the controller directly specified the pod's running node without going through the scheduler.<br>
Until version 1.11, the pods of DaemonSet were scheduled by scheduler before being introduced as alpha features</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 3]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day3/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day3/">
        </link>
        <updated>2020-04-05T21:58:16.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Create a deployment and expose Service with a single command. The deployment and service name is cka-1120, using nginx image, deployment has 2 pods</p>
<h2 id="answer">Answer</h2>
<pre><code>[root@liabio ~]# kubectl run  cka-1120 --replicas 2 --expose=true --port=80 --image=nginx

kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
service/cka-1120 created
deployment.apps/cka-1120 created

[root@liabio ~]# kubectl get all | grep cka-1120
pod/cka-1120-554b9c4798-7jcrb   1/1 Running 0118m
pod/cka-1120-554b9c4798-fpjwj   1/1 Running 0118m
service/cka-1120  ClusterIP 10.108.140.25 &lt;none&gt; 80/TCP           118m
deployment.apps/cka-11202/222118m
 
</code></pre>
<p>The official website provides detailed kubectl usage, located under REFERENCE-&gt;kubectl CLI-&gt;kubectl Commands tab.<br>
https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run</p>
<p>Command <code>kubectl run</code> will create a <code>deployment</code> or <code>job</code> to manage Pods. The command syntax is as follows:</p>
<pre><code>kubectl run NAME --image=image [--env=&quot;key=value&quot;] [--port=port] [--replicas=replicas] [--dry-run=bool] [--overrides=inline-json] [--command] -- [COMMAND] [args...]
</code></pre>
<p>NAME specifies the name of deployment and service;</p>
<p>--replicas abbreviation -r, specify the number of instances, the default is 1;</p>
<p>--expose if true, will create a ClusterIP service, the default is false;</p>
<p>--port indicates the port exposed by the container. If expose is true, the port is also the service port;</p>
<p>--image specifies the image used by the container;</p>
<p>--dry-run is true, only print the object to be sent, and not send it, the default is false.</p>
<p>Create a deployment named cka-1120-01 with environment variables</p>
<pre><code>kubectl run cka-1120-01 --image=nginx --env=&quot;DNS_DOMAIN=cluster.local&quot; --env=&quot;POD_NAMESPACE=default&quot;
</code></pre>
<p>Create a deployment named cka-1120-02 with label</p>
<pre><code>kubectl run cka-1120-02 --image=nginx --labels=&quot;app=nginx,env=prod&quot;
</code></pre>
<p>There is also a <code>--restart</code> parameter, the default is Always, if set to OnFailure, the job will be created; if set to Never, the ordinary Pod will be created.</p>
<pre><code>kubectl run cka-1120-03 --image=nginx --restart=OnFailure

kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.
job.batch/cka-1120-03 created
</code></pre>
<pre><code>kubectl run cka-1120-04 --image=nginx --restart=Never

pod/cka-1120-04 created
</code></pre>
<p><code>--schedule</code> specifies the timing rule of cronjob, if this parameter is specified, cronjob will be created.</p>
<pre><code>kubectl run pi --schedule=&quot;0/5 * * * ?&quot; --image=perl --restart=OnFailure -- perl -Mbignum=bpi -wle 'print bpi(2000)'

cronjob.batch/pi created
</code></pre>
<p>Currently, it is not supported to directly create resource objects such as Statefulset and Daemonset.</p>
<p><strong>What happens after kubectl run is executed?</strong></p>
<p>https://github.com/kubernetes/kubernetes/blob/master/cmd/clicheck/check_cli_conventions.go</p>
<pre><code>func main() {
	var errorCount int

	kubectl := cmd.NewKubectlCommand(os.Stdin, ioutil.Discard, ioutil.Discard)
	errors := cmdsanity.RunCmdChecks(kubectl, cmdsanity.AllCmdChecks, []string{})
	for _, err := range errors {
		errorCount++
		fmt.Fprintf(os.Stderr, &quot;     %d. %s\n&quot;, errorCount, err)
	}
</code></pre>
<p><code>cmd.NewKubectlCommand</code> is to build <code>kubectl</code> and its sub-command line parameters. The final code to execute the business logic is under the &quot;pkg\kubectl&quot; package. Different subcommands: apply, run, create entry corresponding to &quot;pkg\kubectl\cmd&quot;<br>
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kubectl/pkg/cmd/run/run.go</p>
<pre><code>func NewCmdRun(f cmdutil.Factory, streams genericclioptions.IOStreams) *cobra.Command {
	o := NewRunOptions(streams)

	cmd := &amp;cobra.Command{
		Use:                   &quot;run NAME --image=image [--env=\&quot;key=value\&quot;] [--port=port] [--dry-run=server|client] [--overrides=inline-json] [--command] -- [COMMAND] [args...]&quot;,
		DisableFlagsInUseLine: true,
		Short:                 i18n.T(&quot;Run a particular image on the cluster&quot;),
		Long:                  runLong,
		Example:               runExample,
		Run: func(cmd *cobra.Command, args []string) {
			cmdutil.CheckErr(o.Complete(f, cmd))
			cmdutil.CheckErr(o.Run(f, cmd, args))
		},
	}
</code></pre>
<p><code>o.Run(f, cmd, args)</code> will perform a series of checks on the parameters passed by kubectl run, filled with default values</p>
<pre><code>	var createdObjects = []*RunObject{}
	runObject, err := o.createGeneratedObject(f, cmd, generator, names, params, cmdutil.GetFlagString(cmd, &quot;overrides&quot;), namespace)
	if err != nil {
		return err
	}
	createdObjects = append(createdObjects, runObject)

	allErrs := []error{}
	if o.Expose {
		serviceGenerator := cmdutil.GetFlagString(cmd, &quot;service-generator&quot;)
		if len(serviceGenerator) == 0 {
			return cmdutil.UsageErrorf(cmd, &quot;No service generator specified&quot;)
		}
		serviceRunObject, err := o.generateService(f, cmd, serviceGenerator, params, namespace)
		if err != nil {
			allErrs = append(allErrs, err)
		} else {
			createdObjects = append(createdObjects, serviceRunObject)
		}
	}
</code></pre>
<p><code>o.generateService</code> Generate deployment, cronjob, job, pod and other resource objects according to different generators, and send creation requests to apiserver.</p>
<p>If expose is set to true, the same call o.createGeneratedObject is used to generate and create a service.</p>
<pre><code>func (o *RunOptions) createGeneratedObject(f cmdutil.Factory, cmd *cobra.Command, generator generate.Generator, names []generate.GeneratorParam, params map[string]interface{}, overrides, namespace string) (*RunObject, error) {
	err := generate.ValidateParams(names, params)
	if err != nil {
		return nil, err
	}

	// TODO: Validate flag usage against selected generator. More tricky since --expose was added.
	obj, err := generator.Generate(params)
	if err != nil {
		return nil, err
	}
</code></pre>
<p><code>generator.Generate(params)</code> generates different resource objects according to different generator implementations.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 2]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-questions-day-2/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-questions-day-2/">
        </link>
        <updated>2020-04-05T03:19:12.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Under the Kubernetes PVC + PV system, What components need to participate the volume plug-in implemented by CSI, from the pv being dynamically created to pv being used by pods?</p>
<p>A.</p>
<pre><code>PersistentVolumeController + CSI-Provisoner + CSI controller plugin
</code></pre>
<p>B.</p>
<pre><code>AttachDetachController + CSI-Attacher + CSI controller plugin
</code></pre>
<p>C.</p>
<pre><code>Kubelet + CSI node plugin
</code></pre>
<h2 id="answer-a-b-and-c">Answer: A, B and C</h2>
<p>K8s uses <code>PVC</code> to describe the size of the persistent storage that Pod wants to use, read and write permissions, etc., which are generally created by developers; <code>PV</code> describes specific storage types, storage addresses, mounting directories, etc., generally by Ops personnel create. Instead of writing the volume information directly in the pod. First, it can make the development and operation and maintenance responsibilities clear. Second, the use of <code>PVC</code> and <code>PV</code> mechanisms can be well extended to support different storage implementations on the market, such as k8s v1.10 version for Local Persistent Volume.</p>
<p>Let's try to understand the overall process from Pod creation to volume availability.</p>
<p>The user submits a request to create a pod, and <code>PersistentVolumeController</code> finds that the pod claims to use <code>PVC</code>, and it will help it find a <code>PV</code> pair.</p>
<p>If there is no ready-made <code>PV</code>, go to the corresponding <code>StorageClass</code>, help it create a new <code>PV</code>, and then complete the binding with <code>PVC</code>.</p>
<p>The newly created <code>PV</code> is still just an API object, and it needs to undergo &quot;two-stage processing&quot; before it can be used as the &quot;persistent volume&quot; on the host machine:</p>
<ul>
<li>
<p>In the first phase, <code>AttachDetachController</code> running on the master is responsible for attaching the <code>PV</code> and mounting the remote disk for the host;</p>
</li>
<li>
<p>The second stage is to run inside the kubelet component on each node, mount the remote disk attached in the first step to the host directory. This control loop is called <code>VolumeManagerReconciler</code>, which runs on a separate Goroutine and does not block the kubelet main control loop.</p>
</li>
</ul>
<p>After completing these two steps, the &quot;persistent volume&quot; corresponding to the PV is ready, and the POD can be started normally, and the &quot;persistent volume&quot; is mounted on the specified path in the container.</p>
<p>k8s supports writing its own storage plug-in <code>FlexVolume</code> and <code>CSI</code>. Either way, you need to go through &quot;two-stage processing.&quot; FlexVolume has more limitations than CSI. Generally, we use <code>CSI</code> to dock storage.</p>
<p>The design idea of ​​the <code>CSI</code> plug-in system strips this Provision stage (dynamically creating <code>PV</code>) and part of the storage management functions in Kubernetes from the main code to make it into several separate components. These components will monitor changes in storage-related events in Kubernetes, such as the creation of <code>PVC</code>, through the Watch API to perform specific storage management actions.</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586270614137.png" alt="" loading="lazy"></figure>
<p>The three independent external components (External Components) in the CSI storage plug-in system in the above figure, namely: <code>Driver Registrar</code>, <code>External Provisioner</code> and <code>External Attacher</code>, correspond to some storage management functions stripped from the Kubernetes project.</p>
<p>We need to implement a binary of Custom Components, which will provide three services in gRpc mode: <code>CSI Identity</code>, <code>CSI Controller</code>, and <code>CSI Node</code>.</p>
<p><code>Driver Registrar</code> component, which is responsible for registering the plug-in in kubelet; <code>Driver Registrar</code> calls CSI Identity service to obtain plug-in information.</p>
<p>The <code>External Provisioner</code> component listens to the PVC objects in the APIServer. When a PVC is created, it will call the CreateVolume method of the CSI Controller to create the corresponding PV;</p>
<p>The <code>External Attacher</code> component is responsible for the Attach phase. The Mount phase is completed by the <code>VolumeManagerReconciler</code> control loop in the kubelet directly calling the CSI Node service.</p>
<p>After the two stages are completed, kubelet passes the mount parameter to docker to create and start the container.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes Questions -- Day 1]]></title>
        <id>https://jiaming-hu.com/post/kubernetes-question-day1/</id>
        <link href="https://jiaming-hu.com/post/kubernetes-question-day1/">
        </link>
        <updated>2020-04-04T00:52:09.000Z</updated>
        <content type="html"><![CDATA[<h2 id="question">Question</h2>
<p>Which of the following <code>Daemonset</code> yaml are correct:</p>
<p>A.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels: k8s-app: fluentd-logging 
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      containers:
      - name: fluentd-elasticsearch
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
        restartPolicy: Never
</code></pre>
<p>B.</p>
<pre><code class="language-yaml">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: default
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata: 
      labels: 
        name: fluentd-elasticsearch
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Onfailure
</code></pre>
<p>C.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1 
        restartPolicy: Always
</code></pre>
<p>D.</p>
<pre><code class="language-yaml">apiVersion: apps/v1 
kind: DaemonSet 
metadata: 
  name: fluentd-elasticsearch 
  namespace: default 
  labels: 
    k8s-app: fluentd-logging 
spec: 
  selector: 
    matchLabels: 
      name: fluentd-elasticsearch 
  template: 
    metadata: 
      labels: 
        name: fluentd-elasticsearch 
    spec: 
      containers: 
      - name: fluentd-elasticsearch 
        image: gcr.io/fluentd-elasticsearch/fluentd:v2.5.1
</code></pre>
<h2 id="answer-c-and-d"><strong>Answer: C and D</strong></h2>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</p>
<figure data-type="image" tabindex="1"><img src="https://jiaming-hu.com/post-images/1586221558403.jpeg" alt="" loading="lazy"></figure>
<p><code>RestartPolicy</code> field, optional values are <code>Always</code>, <code>OnFailure</code>, and <code>Never</code>. The default is Always. There can be multiple containers in a Pod, and restartPolicy applies to all containers in the Pod. The purpose of restartPolicy is to let kubelet restart the failed container.</p>
<p>In addition, the <code>restartPolicy</code> of <code>Deployment</code> and <code>Statefulset</code> must also be <code>Always</code> to ensure that the pod exits abnormally, or the health check livenessProbe fails and the kubelet restarts the container.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/deployment</p>
<p><code>Job</code> and <code>CronJob</code> are pods that run once, and <code>restartPolicy</code> can only be <code>OnFailure</code> or <code>Never</code>, to ensure that the container will not restart after the execution is completed.</p>
<p>https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Functional Options Pattern in Go]]></title>
        <id>https://jiaming-hu.com/post/functional-options-pattern-in-go/</id>
        <link href="https://jiaming-hu.com/post/functional-options-pattern-in-go/">
        </link>
        <updated>2020-04-01T13:43:20.000Z</updated>
        <content type="html"><![CDATA[<!-- more -->
<blockquote>
<p>reference <a href="https://halls-of-valhalla.org/beta/articles/functional-options-pattern-in-go,54/">functional-options-pattern-in-go</a></p>
</blockquote>
<p>One of the many issues you'll encounter as a Golang developer is trying to make parameters to a function optional. It's a pretty common use case that you have some object which should work out-of-the-box using some basic default settings, and you may occasionally want to provide some more detailed configuration.</p>
<p>In many languages this is easy; in C-family languages, you can provide multiple versions of the same function with different numbers of arguments; in languages like PHP, you can give parameters a default value and omit them when calling the method. But in Golang you can't do either of those. So how do you create a function which has some additional configuration a user can specify if they want, but only if they want to?</p>
<p>There are a number of possible ways to do this, but most are pretty unsatisfactory, either requiring a lot of additional checking and validating in the code on the service-side, or extra work for the client by passing in additional parameters which they don't care about.</p>
<p>I'll walk through some of the different options and show why each is suboptimal and then we'll build our way up to our final, clean solution: the Functional Options Pattern.</p>
<p>Let's take a look at an example. Let's say we have some service called StuffClient which does some stuff and has a two configuration options (timeout and retries):</p>
<pre><code>type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
</code></pre>
<p>That struct is private, so we should provide some sort of constructor for it:</p>
<pre><code>func NewStuffClient(conn Connection, timeout, retries int) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: timeout,
        retries: retries,
    }
}
</code></pre>
<p>Hmm, but now we always have to provide the timeout and retries every time we call <code>NewStuffClient</code>. And most of the time we'll want to just use the default values. We can't define multiple versions of <code>NewStuffClient</code> with different numbers of parameters or else we'll get a compile error like &quot;NewStuffClient redeclared in this block&quot;.</p>
<p>One option would be to create another constructor with a different name like:</p>
<pre><code>func NewStuffClient(conn Connection) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: DEFAULT_TIMEOUT,
        retries: DEFAULT_RETRIES,
    }
}
func NewStuffClientWithOptions(conn Connection, timeout, retries int) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: timeout,
        retries: retries,
    }
}
</code></pre>
<p>But that's kind of crappy. We can do better than that. What if we passed in a config object:</p>
<pre><code>type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func NewStuffClient(conn Connection, options StuffClientOptions) StuffClient {
    return &amp;stuffClient{
        conn:    conn,
        timeout: options.Timeout,
        retries: options.Retries,
    }
}
</code></pre>
<p>So what's the solution? The nicest way to solve this dilemma is with the Functional Options Pattern, making use of Go's convenient support of closures. Let's keep this StuffClientOptions we defined above, but we'll add some things to it:</p>
<pre><code>type StuffClientOption func(*StuffClientOptions)
type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func WithRetries(r int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Timeout = t
    }
}
</code></pre>
<p>Clear as mud right? What's happening here exactly? Basically we have our struct defining the available options for our StuffClient. Additionally now we've defined something called StuffClientOption (singular this time) which is just a function which accepts our options struct as a parameter. We've defined a couple of functions additionally called WithRetries and WithTimeout which return a closure. Now comes the magic:</p>
<pre><code>var defaultStuffClientOptions = StuffClientOptions{
    Retries: 3,
    Timeout: 2,
}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    options := defaultStuffClientOptions
    for _, o := range opts {
        o(&amp;options)
    }
    return &amp;stuffClient{
        conn:    conn,
        timeout: options.Timeout,
        retries: options.Retries,
    }
}
</code></pre>
<p>We've defined now an additional unexposed variable containing our default options, and we've adjusted our constructor now to instead accept a variadic parameter. We then iterate over that list of StuffClientOption (singular) and for each of them, we apply the returned closure to our options variable (and recall that those closures accept an StuffClientOptions variable and simply modify the option value on it).</p>
<p>Now all we have to do to use it is this:</p>
<pre><code>x := NewStuffClient(Connection{})
fmt.Println(x) // prints &amp;{{} 2 3}
x = NewStuffClient(
    Connection{},
    WithRetries(1),
)
fmt.Println(x) // prints &amp;{{} 2 1}
x = NewStuffClient(
    Connection{},
    WithRetries(1),
    WithTimeout(1),
)
fmt.Println(x) // prints &amp;{{} 1 1}
</code></pre>
<p>That looks pretty nice and usable now. And the nice part about it is that we can very easily add new options any time we want with only a very minimal amount of change we need to make to the code.</p>
<p>Putting it all together we have something like this:</p>
<pre><code>var defaultStuffClientOptions = StuffClientOptions{
    Retries: 3,
    Timeout: 2,
}
type StuffClientOption func(*StuffClientOptions)
type StuffClientOptions struct {
    Retries int //number of times to retry the request before giving up
    Timeout int //connection timeout in seconds
}
func WithRetries(r int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *StuffClientOptions) {
        o.Timeout = t
    }
}
type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
type Connection struct {}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    options := defaultStuffClientOptions
    for _, o := range opts {
        o(&amp;options)
    }
        return &amp;stuffClient{
            conn:    conn,
            timeout: options.Timeout,
            retries: options.Retries,
        }
}
func (c stuffClient) DoStuff() error {
    return nil
}
</code></pre>
<p>If you want to try it out yourself, check it out on the <a href="https://play.golang.org/p/VcWqWcAEyz">Go Playground</a>.</p>
<p>But this could be simplified even more by removing the StuffClientOptions struct and applying the options directly to our StuffClient.</p>
<pre><code>var defaultStuffClient = stuffClient{
    retries: 3,
    timeout: 2,
}
type StuffClientOption func(*stuffClient)
func WithRetries(r int) StuffClientOption {
    return func(o *stuffClient) {
        o.retries = r
    }
}
func WithTimeout(t int) StuffClientOption {
    return func(o *stuffClient) {
        o.timeout = t
    }
}
type StuffClient interface {
    DoStuff() error
}
type stuffClient struct {
    conn    Connection
    timeout int
    retries int
}
type Connection struct{}
func NewStuffClient(conn Connection, opts ...StuffClientOption) StuffClient {
    client := defaultStuffClient
    for _, o := range opts {
        o(&amp;client)
    }
    
    client.conn = conn
    return client
}
func (c stuffClient) DoStuff() error {
    return nil
}
</code></pre>
<p>Try it out <a href="https://play.golang.org/p/Z5P5Om4KDL">here</a>. In the case of our example, where we're just applying the config directly to our struct, it makes no sense to have an extra config struct in the middle. However, note that in many cases you may still want to use the config struct from the previous example; for instance, if your constructor is using the config options to perform some operations but then not storing them into the struct, or if they get passed around to other places. The config struct variant is the more generic implementation.</p>
<p>Credit to <a href="https://commandcenter.blogspot.com/2014/01/self-referential-functions-and-design.html">Rob Pike</a> and <a href="https://dave.cheney.net/2014/10/17/functional-options-for-friendly-apis">Dave Cheney</a> for popularizing this design pattern.</p>
]]></content>
    </entry>
</feed>